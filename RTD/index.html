<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>WOODS documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="canonical" href="https://woods.readthedocs.io/en/latest/index.html" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> WOODS
          </a>
              <div class="version">
                latest
              </div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-installation">Quick Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#installing-requirements">Installing requirements</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#with-conda">With Conda</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#with-venv">With venv</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#clone-locally">Clone locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#run-tests">Run tests</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-downloading_datasets">Downloading the data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#direct-preprocessed-download">Direct Preprocessed Download</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#source-download-and-preprocess">Source Download and Preprocess</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#datasets-info">Datasets Info</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-running_a_sweep">Running a  Sweep</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#running-the-sweep">Running the sweep</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#compiling-the-results">Compiling the results</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#advanced-usage">Advanced usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-adding_an_algorithm">Adding an Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#defining-the-algorithm">Defining the Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#adding-necessary-pieces">Adding necessary pieces</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#run-some-tests">Run some tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#try-the-algorithm">Try the algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#run-a-sweep">Run a sweep</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-adding_a_dataset">Adding a Dataset</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#defining-the-algorithm">Defining the Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#adding-necessary-pieces">Adding necessary pieces</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#run-some-tests">Run some tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#try-the-algorithm">Try the algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#run-a-sweep">Run a sweep</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-contributing">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-woods">WOODS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.command_launchers">woods.command_launchers module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.datasets">woods.datasets module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.hyperparams">woods.hyperparams module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.model_selection">woods.model_selection module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.models">woods.models module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.objectives">woods.objectives module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.train">woods.train module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.utils">woods.utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-woods.scripts">woods.scripts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#document-woods.scripts.compile_results">woods.scripts.compile_results module</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#document-woods.scripts.download">woods.scripts.download module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#document-woods.scripts.fetch_and_preprocess">woods.scripts.fetch_and_preprocess module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#document-woods.scripts.hparams_sweep">woods.scripts.hparams_sweep module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#document-woods.scripts.main">woods.scripts.main module</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#document-woods.scripts.visualize_results">woods.scripts.visualize_results module</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">WOODS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>WOODS  documentation</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://anonymous.4open.science/r/WOODS/README.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="woods">
<h1>WOODS<a class="headerlink" href="#woods" title="Permalink to this headline"></a></h1>
<a class="reference internal image-reference" href="_images/banner.png"><img alt="_images/banner.png" class="align-center" src="_images/banner.png" style="width: 600px;" /></a>
<p>WOODS is a project aimed at investigating the implications of Out-of-Distribution generalization problems in sequential data along with it’s possible solution. To that goal, we offer a DomainBed-like suite to test domain generalization algorithms on our WILDS-like set of sequential data benchmarks inspired from real world problems of a wide array of common modalities in modern machine learning.</p>
<div class="toctree-wrapper compound">
<span id="document-installation"></span><section id="quick-installation">
<h2>Quick Installation<a class="headerlink" href="#quick-installation" title="Permalink to this headline"></a></h2>
<p>WOODS is still under active developpement so it is still only available by cloning the repository on your local machine.</p>
<section id="installing-requirements">
<h3>Installing requirements<a class="headerlink" href="#installing-requirements" title="Permalink to this headline"></a></h3>
<section id="with-conda">
<h4>With Conda<a class="headerlink" href="#with-conda" title="Permalink to this headline"></a></h4>
<p>First, have conda installed on your machine (see their <a class="reference external" href="https://docs.anaconda.com/anaconda/install/">installation page</a> if that is not the case). Then create a conda environment with the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>conda create --name woods <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.7
</pre></div>
</div>
<p>Then activate the environment with the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>conda activate woods
</pre></div>
</div>
</section>
<section id="with-venv">
<h4>With venv<a class="headerlink" href="#with-venv" title="Permalink to this headline"></a></h4>
<p>You can use the python virtual environment manager <a class="reference external" href="https://virtualenv.pypa.io/en/latest/">virtualenv</a> to create a virtual environment for the project. IMPORTANT: Make sure you are using python &gt;3.7.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>virtualenv /path/to/woods/env
</pre></div>
</div>
<p>Then activate the virtual environment with the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span> /path/to/env/woods/bin/activate
</pre></div>
</div>
</section>
</section>
<section id="clone-locally">
<h3>Clone locally<a class="headerlink" href="#clone-locally" title="Permalink to this headline"></a></h3>
<p>Once you’ve created the virtual environment, clone the repository.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/jc-audet/WOODS.git
<span class="nb">cd</span> WOODS
</pre></div>
</div>
<p>Then install the requirements with the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip install -r requirements.txt
</pre></div>
</div>
</section>
<section id="run-tests">
<h3>Run tests<a class="headerlink" href="#run-tests" title="Permalink to this headline"></a></h3>
<p>Run the tests to make sure everything is in order. More tests are coming soon.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pytest
</pre></div>
</div>
</section>
</section>
<span id="document-downloading_datasets"></span><section id="downloading-the-data">
<h2>Downloading the data<a class="headerlink" href="#downloading-the-data" title="Permalink to this headline"></a></h2>
<p>Before running any training run, we need to make sure we have the data to train on.</p>
<section id="direct-preprocessed-download">
<h3>Direct Preprocessed Download<a class="headerlink" href="#direct-preprocessed-download" title="Permalink to this headline"></a></h3>
<p>The repository offers direct download to the preprocessed data which is the quickest and most efficient way to get started. To download the preprocessed data, run the download module of the woods.scripts package and specify the dataset you want to download:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.download DATASET<span class="se">\</span>
        --data_path ./path/to/data/directory
</pre></div>
</div>
</section>
<section id="source-download-and-preprocess">
<h3>Source Download and Preprocess<a class="headerlink" href="#source-download-and-preprocess" title="Permalink to this headline"></a></h3>
<p>For the sake of transparency, WOODS also offers the preprocessing scripts we took for all datasets in the preprecessing module of the woods.scripts package. You can also use the same module to download the raw data from the original source and run preprocessing yourself on it. DISCLAIMER: Some of the datasets take a long time to preprocess, especially the EEG datasets.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.fetch_and_preprocess DATASET<span class="se">\</span>
        --data_path ./path/to/data/directory
</pre></div>
</div>
</section>
<section id="datasets-info">
<h3>Datasets Info<a class="headerlink" href="#datasets-info" title="Permalink to this headline"></a></h3>
<p>The following table lists the available datasets and their corresponding raw and preprocessed sizes.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Datasets</th>
<th>Modality</th>
<th>Requires Download</th>
<th>Preprocessed Size</th>
<th>Raw Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic_Fourier</td>
<td>1D Signal</td>
<td>No</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Spurious_Fourier</td>
<td>1D Signal</td>
<td>No</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>TMNIST</td>
<td>Video</td>
<td>Yes, but done automatically</td>
<td>0.11 GB</td>
<td>-</td>
</tr>
<tr>
<td>TCMNIST_seq</td>
<td>Video</td>
<td>Yes, but done automatically</td>
<td>0.11 GB</td>
<td>-</td>
</tr>
<tr>
<td>TCMNSIT_step</td>
<td>Video</td>
<td>Yes, but done automatically</td>
<td>0.11 GB</td>
<td>-</td>
</tr>
<tr>
<td>CAP</td>
<td>EEG</td>
<td>Yes</td>
<td>9.1 GB</td>
<td>40.1 GB</td>
</tr>
<tr>
<td>SEDFx</td>
<td>EEG</td>
<td>Yes</td>
<td>10.7 GB</td>
<td>8.1 GB</td>
</tr>
<tr>
<td>MI</td>
<td>EEG</td>
<td>Yes</td>
<td>3.0GB</td>
<td>13.5 GB</td>
</tr>
<tr>
<td>LSA64</td>
<td>Video</td>
<td>Yes</td>
<td>0.26 GB</td>
<td>1.5 GB</td>
</tr>
<tr>
<td>HAR</td>
<td>Sensor</td>
<td>Yes</td>
<td>0.16 GB</td>
<td>3.1 GB</td>
</tr>
</tbody>
</table></section>
</section>
<span id="document-running_a_sweep"></span><section id="running-a-sweep">
<h2>Running a  Sweep<a class="headerlink" href="#running-a-sweep" title="Permalink to this headline"></a></h2>
<p>In WOODS, we evaluate the performance of a domain generalization algorithm by running a sweep over the hyper parameters definition space and then performing model selection on the training runs conducted during the sweep.</p>
<section id="running-the-sweep">
<h3>Running the sweep<a class="headerlink" href="#running-the-sweep" title="Permalink to this headline"></a></h3>
<p>Once we have the data, we can start running the sweep. The hparams_sweep module of the woods.scripts package provides the command line interface to create the list of jobs to run, which is then passed to the command launcher to launch all jobs. The list of jobs includes all of the necessary training runs to get the results from all trial seeds, and hyper parameter seeds for a given algorithm, dataset and test domain.</p>
<p>All datasets have the <code class="docutils literal notranslate"><span class="pre">SWEEP_ENVS</span></code> attributes that defines which test environments are included in the sweep. For example, the <code class="docutils literal notranslate"><span class="pre">SWEEP_ENVS</span></code> attribute for the <code class="docutils literal notranslate"><span class="pre">Spurious</span> <span class="pre">Fourier</span></code> dataset is only 1 test domain while for most real datasets <code class="docutils literal notranslate"><span class="pre">SWEEP_ENVS</span></code> consists of all domains.</p>
<p>In other words, for every combination of (algorithm, dataset, test environment) we train 20 different hyper parameter configurations on which we investigate 3 different trial seeds. This means that for every combination of (algorithm, dataset, test environment) we run 20 * 3 = 60 training runs.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.hparams_sweep <span class="se">\</span>
        --dataset Spurious_Fourier TCMNIST_seq <span class="se">\</span>
        --objective ERM IRM <span class="se">\</span>
        --save_path ./results <span class="se">\</span>
        --launcher <span class="nb">local</span>
</pre></div>
</div>
<p>Here we are using the local launcher to run the jobs locally, which is the simplest launcher. We also offer other lauchers in the command_launcher module, such as slurm_launcher which is a parallel job launcher for the SLURM workload manager.</p>
</section>
<section id="compiling-the-results">
<h3>Compiling the results<a class="headerlink" href="#compiling-the-results" title="Permalink to this headline"></a></h3>
<p>Once the sweep is finished, we can compile the results. The compile_results module of the woods.scripts package provides the command line interface to compile the results. The –latex option is used to generate the latex table.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.compile_results <span class="se">\</span>
        --results_dir path/to/results <span class="se">\</span>
        --latex
</pre></div>
</div>
<p>It is also possible to compile the results from multiple directories containing complementary sweeps results. This will put all of those results in the same table.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.compile_results <span class="se">\</span>
        --results_dir path/to/results/1 path/to/results/2 path/to/results/3 <span class="se">\</span>
        --latex
</pre></div>
</div>
<p>There are other mode of operation for the compile_results module, such as <code class="docutils literal notranslate"><span class="pre">--mode</span> <span class="pre">IID</span></code> which takes results from a sweep with no test environment and report the results for each test environment separately.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.compile_results <span class="se">\</span>
        --results_dir path/to/results/1 path/to/results/2 path/to/results/3 <span class="se">\</span>
        --mode IID
</pre></div>
</div>
<p>There is also <code class="docutils literal notranslate"><span class="pre">--mode</span> <span class="pre">summary</span></code> which reports the average results for every dataset of all objectives in the sweep.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.compile_results <span class="se">\</span>
        --results_dir path/to/results/1 path/to/results/2 path/to/results/3 <span class="se">\</span>
        --mode summary
</pre></div>
</div>
<p>You can also use the <code class="docutils literal notranslate"><span class="pre">--mode</span> <span class="pre">hparams</span></code> which reports the hparams of the model chosen by model selection</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.compile_results <span class="se">\</span>
        --results_dir path/to/results/1 path/to/results/2 path/to/results/3 <span class="se">\</span>
        --mode hparams
</pre></div>
</div>
</section>
<section id="advanced-usage">
<h3>Advanced usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h3>
<p>If 60 jobs is too many jobs for you available compute, or too few for you experiments you can change the number of seeds investigated, you can call the <code class="docutils literal notranslate"><span class="pre">--n_hparams</span></code> and <code class="docutils literal notranslate"><span class="pre">--n_trials</span></code> argument.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.hparams_sweep <span class="se">\</span>
        --dataset Spurious_Fourier TCMNIST_seq <span class="se">\</span>
        --objective ERM IRM <span class="se">\</span>
        --save_path ./results <span class="se">\</span>
        --launcher <span class="nb">local</span> <span class="se">\</span>
        --n_hparams <span class="m">10</span> <span class="se">\</span>
        --n_trials <span class="m">1</span>
</pre></div>
</div>
<p>If some of the test environment of a dataset is not of interest to you, you can specify which test environment you want to investigate using the <code class="docutils literal notranslate"><span class="pre">--unique_test_env</span></code> argument</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.hparams_sweep <span class="se">\</span>
        --dataset Spurious_Fourier TCMNIST_seq <span class="se">\</span>
        --objective ERM IRM <span class="se">\</span>
        --save_path ./results <span class="se">\</span>
        --launcher <span class="nb">local</span> <span class="se">\</span>
        --unique_test_env <span class="m">0</span>
</pre></div>
</div>
<p>You can run a sweep with no test environment by specifying the <code class="docutils literal notranslate"><span class="pre">--unique_test_env</span></code> argument as <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.hparams_sweep <span class="se">\</span>
        --dataset Spurious_Fourier TCMNIST_seq <span class="se">\</span>
        --objective ERM IRM <span class="se">\</span>
        --save_path ./results <span class="se">\</span>
        --launcher <span class="nb">local</span> <span class="se">\</span>
        --unique_test_env None
</pre></div>
</div>
</section>
</section>
<span id="document-adding_an_algorithm"></span><section id="adding-an-algorithm">
<h2>Adding an Algorithm<a class="headerlink" href="#adding-an-algorithm" title="Permalink to this headline"></a></h2>
<p>In this section, we will walk through the process of adding an algorithm to the framework.</p>
<section id="defining-the-algorithm">
<h3>Defining the Algorithm<a class="headerlink" href="#defining-the-algorithm" title="Permalink to this headline"></a></h3>
<p>We first define the algorithm by creating a new class in the objectives module. In this example we will add scaled_ERM which is simply ERM with a random scale factor between 0 and max_scale for each environment in a dataset, where max_scale is an hyperparameter of the objective.</p>
<p>Let’s first define the class and its <strong>int</strong> method to initialize the algorithm.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">scaled_ERM</span><span class="p">(</span><span class="n">ERM</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scaled Empirical Risk Minimization (scaled ERM)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">hparams</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">scaled_ERM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">hparams</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_scale</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">[</span><span class="s1">&#39;max_scale&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">train_names</span><span class="p">))</span> 
</pre></div>
</div>
<p>We then need to define the update function, which take a minibatch of data and compute the loss and update the model according to the algorithm definition. Note here that we do not need to define the predict function, as it is already defined in the base class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minibatches_device</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>

        <span class="c1">## Group all inputs and send to device</span>
        <span class="n">all_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">minibatches_device</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">all_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">minibatches_device</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">ts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">PRED_TIME</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">all_x</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="c1">## Reshape the data so the first dimension are environments)</span>
        <span class="n">out_split</span><span class="p">,</span> <span class="n">labels_split</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">all_y</span><span class="p">)</span>

        <span class="n">env_losses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_split</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_split</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">t_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_split</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]):</span>     <span class="c1"># Number of time steps</span>
                <span class="n">env_losses</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_factor</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">out_split</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">t_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">labels_split</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="n">t_idx</span><span class="p">])</span>

        <span class="n">objective</span> <span class="o">=</span> <span class="n">env_losses</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># Back propagate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">objective</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="adding-necessary-pieces">
<h3>Adding necessary pieces<a class="headerlink" href="#adding-necessary-pieces" title="Permalink to this headline"></a></h3>
<p>Now that our algorithm is defined, we can add it to the list of algorithms at the top of the objectives module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">OBJECTIVES</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;ERM&#39;</span><span class="p">,</span>
    <span class="s1">&#39;IRM&#39;</span><span class="p">,</span>
    <span class="s1">&#39;VREx&#39;</span><span class="p">,</span>
    <span class="s1">&#39;SD&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ANDMask&#39;</span><span class="p">,</span>
    <span class="s1">&#39;IGA&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scaled_ERM&#39;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Before being able to use the algorithm, we need to add the hyper parameters related to this algorithm in the hyperparams module. Note: the name of the funtion needs to be the same as the name of the algorithm followed by _hyper.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scaled_ERM_hyper</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; scaled ERM objective hparam definition </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        sample (bool): If &#39;&#39;True&#39;&#39;, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to &#39;&#39;False&#39;&#39; where the default value is chosen.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sample</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;max_scale&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">r</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">10.</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;max_scale&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mf">2.</span>
        <span class="p">}</span>
</pre></div>
</div>
</section>
<section id="run-some-tests">
<h3>Run some tests<a class="headerlink" href="#run-some-tests" title="Permalink to this headline"></a></h3>
<p>We can now run a simple test to check that everything is working as expected</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pytest
</pre></div>
</div>
</section>
<section id="try-the-algorithm">
<h3>Try the algorithm<a class="headerlink" href="#try-the-algorithm" title="Permalink to this headline"></a></h3>
<p>Then we can run a training run to see how the algorithm performs on any dataset</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.main train <span class="se">\</span>
        --dataset Spurious_Fourier <span class="se">\</span>
        --objective scaled_ERM <span class="se">\</span>
        --test_env <span class="m">0</span> <span class="se">\</span>
        --data_path ./data
</pre></div>
</div>
</section>
<section id="run-a-sweep">
<h3>Run a sweep<a class="headerlink" href="#run-a-sweep" title="Permalink to this headline"></a></h3>
<p>Finally, we can run a sweep to see how the algorithm performs on all the datasets</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.hparams_sweep <span class="se">\</span>
        --objective scaled_ERM <span class="se">\</span>
        --dataset Spurious_Fourier <span class="se">\</span>
        --data_path ./data <span class="se">\</span>
        --launcher dummy
</pre></div>
</div>
</section>
</section>
<span id="document-adding_a_dataset"></span><section id="adding-a-dataset">
<h2>Adding a Dataset<a class="headerlink" href="#adding-a-dataset" title="Permalink to this headline"></a></h2>
<p>In this section, we will walk through the process of adding an dataset to the framework.</p>
<section id="defining-the-algorithm">
<h3>Defining the Algorithm<a class="headerlink" href="#defining-the-algorithm" title="Permalink to this headline"></a></h3>
<p>We first define the dataset by creating a new class in the datasets module. In this example we will add flat_MNIST which is the MNIST dataset, but the image is fed to a sequential model pixel by pixel and the environments are different orders of the pixels.</p>
<p>First let’s define the dataset class and its <strong>init</strong> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">flat_MNIST</span><span class="p">(</span><span class="n">Multi_Domain_Dataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Class for flat MNIST dataset</span>

<span class="sd">    Each sample is a sequence of 784 pixels.</span>
<span class="sd">    The task is to predict the digit</span>

<span class="sd">    Args:</span>
<span class="sd">        flags (argparse.Namespace): argparse of training arguments</span>

<span class="sd">    Note:</span>
<span class="sd">        The MNIST dataset needs to be downloaded, this is automaticaly done if the dataset isn&#39;t in the given data_path</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">## Dataset parameters</span>
    <span class="n">SETUP</span> <span class="o">=</span> <span class="s1">&#39;seq&#39;</span>
    <span class="n">TASK</span> <span class="o">=</span> <span class="s1">&#39;classification&#39;</span>
    <span class="n">SEQ_LEN</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>
    <span class="n">PRED_TIME</span> <span class="o">=</span> <span class="p">[</span><span class="mi">783</span><span class="p">]</span>
    <span class="n">INPUT_SHAPE</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">OUTPUT_SIZE</span> <span class="o">=</span> <span class="mi">10</span>

    <span class="c1">## Environment parameters</span>
    <span class="n">ENVS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;forwards&#39;</span><span class="p">,</span> <span class="s1">&#39;backwards&#39;</span><span class="p">,</span> <span class="s1">&#39;scrambled&#39;</span><span class="p">]</span>
    <span class="n">SWEEP_ENVS</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ENVS</span><span class="p">)))</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flags</span><span class="p">,</span> <span class="n">training_hparams</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">flags</span><span class="o">.</span><span class="n">test_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">flags</span><span class="o">.</span><span class="n">test_env</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ENVS</span><span class="p">),</span> <span class="s2">&quot;Test environment chosen is not valid&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;You don&#39;t have any test environment&quot;</span><span class="p">)</span>

        <span class="c1"># Save stuff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_env</span> <span class="o">=</span> <span class="n">flags</span><span class="o">.</span><span class="n">test_env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_balance</span> <span class="o">=</span> <span class="n">training_hparams</span><span class="p">[</span><span class="s1">&#39;class_balance&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">training_hparams</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>

        <span class="c1">## Import original MNIST data</span>
        <span class="n">MNIST_tfrm</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span> <span class="p">])</span>

        <span class="c1"># Get MNIST data</span>
        <span class="n">train_ds</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">flags</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">MNIST_tfrm</span><span class="p">)</span> 
        <span class="n">test_ds</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">flags</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">MNIST_tfrm</span><span class="p">)</span> 

        <span class="c1"># Concatenate all data and labels</span>
        <span class="n">MNIST_images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">train_ds</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()))</span>
        <span class="n">MNIST_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">train_ds</span><span class="o">.</span><span class="n">targets</span><span class="p">,</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">targets</span><span class="p">))</span>

        <span class="c1"># Create sequences of 784 pixels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TCMNIST_images</span> <span class="o">=</span> <span class="n">MNIST_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">MNIST_labels</span> <span class="o">=</span> <span class="n">MNIST_labels</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Make the color datasets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loaders</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">val_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_loaders</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span> 
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ENVS</span><span class="p">):</span>

            <span class="c1"># Choose data subset</span>
            <span class="n">images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">TCMNIST_images</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ENVS</span><span class="p">),</span><span class="o">...</span><span class="p">]</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">MNIST_labels</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ENVS</span><span class="p">),</span><span class="o">...</span><span class="p">]</span>

            <span class="c1"># Apply environment definition</span>
            <span class="k">if</span> <span class="n">e</span> <span class="o">==</span> <span class="s1">&#39;forwards&#39;</span><span class="p">:</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span>
            <span class="k">elif</span> <span class="n">e</span> <span class="o">==</span> <span class="s1">&#39;backwards&#39;</span><span class="p">:</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="n">e</span> <span class="o">==</span> <span class="s1">&#39;scrambled&#39;</span><span class="p">:</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">[:,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">),</span> <span class="p">:]</span>

            <span class="c1"># Make Tensor dataset and the split</span>
            <span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">in_dataset</span><span class="p">,</span> <span class="n">out_dataset</span> <span class="o">=</span> <span class="n">make_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">flags</span><span class="o">.</span><span class="n">holdout_fraction</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_env</span><span class="p">:</span>
                <span class="n">in_loader</span> <span class="o">=</span> <span class="n">InfiniteLoader</span><span class="p">(</span><span class="n">in_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">training_hparams</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_in&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_loaders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">in_loader</span><span class="p">)</span>
            
            <span class="n">fast_in_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">in_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">N_WORKERS</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">val_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_in&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">val_loaders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fast_in_loader</span><span class="p">)</span>
            <span class="n">fast_out_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">out_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">N_WORKERS</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">val_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_out&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">val_loaders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fast_out_loader</span><span class="p">)</span>

        <span class="c1"># Define loss function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_prob</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_class_weight</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">training_hparams</span><span class="p">[</span><span class="s1">&#39;device&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>Note:
you are required to define the following variables:
* SETUP
* SEQ_LEN
* PRED_TIME
* INPUT_SHAPE
* OUTPUT_SIZE
* ENVS
* SWEEP_ENVS
you are also encouraged to redefine the following variables:
* N_STEPS
* N_WORKERS
* CHECKPOINT_FREQ</p>
</section>
<section id="adding-necessary-pieces">
<h3>Adding necessary pieces<a class="headerlink" href="#adding-necessary-pieces" title="Permalink to this headline"></a></h3>
<p>Now that our algorithm is defined, we can add it to the list of algorithms at the top of the objectives module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DATASETS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># 1D datasets</span>
    <span class="s1">&#39;Basic_Fourier&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Spurious_Fourier&#39;</span><span class="p">,</span>
    <span class="c1"># Small images</span>
    <span class="s2">&quot;TMNIST&quot;</span><span class="p">,</span>
    <span class="c1"># Small correlation shift dataset</span>
    <span class="s2">&quot;TCMNIST_seq&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TCMNIST_step&quot;</span><span class="p">,</span>
    <span class="c1">## EEG Dataset</span>
    <span class="s2">&quot;CAP_DB&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SEDFx_DB&quot;</span><span class="p">,</span>
    <span class="c1">## Financial Dataset</span>
    <span class="s2">&quot;StockVolatility&quot;</span><span class="p">,</span>
    <span class="c1">## Sign Recognition</span>
    <span class="s2">&quot;LSA64&quot;</span><span class="p">,</span>
    <span class="c1">## Activity Recognition</span>
    <span class="s2">&quot;HAR&quot;</span><span class="p">,</span>
    <span class="c1">## Example</span>
    <span class="s2">&quot;flat_MNIST&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Before being able to use the dataset, we need to add the hyper parameters related to this dataset in the hyperparams module. Note: the name of the funtion needs to be the same as the name of the dataset followed by _train and _model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">flat_MNIST_train</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; flat_MNIST model hparam definition </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        sample (bool): If &#39;&#39;True&#39;&#39;, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to &#39;&#39;False&#39;&#39; where the default value is chosen.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sample</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;class_balance&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mi">10</span><span class="o">**</span><span class="n">r</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">),</span>
            <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">r</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
        <span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;class_balance&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
            <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mi">64</span>
        <span class="p">}</span>

<span class="k">def</span> <span class="nf">flat_MNIST_model</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot; flat_MNIST model hparam definition </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        sample (bool): If &#39;&#39;True&#39;&#39;, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to &#39;&#39;False&#39;&#39; where the default value is chosen.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="s1">&#39;LSTM&#39;</span><span class="p">,</span>
        <span class="s1">&#39;hidden_depth&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> 
        <span class="s1">&#39;hidden_width&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="s1">&#39;recurrent_layers&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s1">&#39;state_size&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="mi">32</span>
    <span class="p">}</span>
</pre></div>
</div>
</section>
<section id="run-some-tests">
<h3>Run some tests<a class="headerlink" href="#run-some-tests" title="Permalink to this headline"></a></h3>
<p>We can now run a simple test to check that everything is working as expected</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pytest
</pre></div>
</div>
</section>
<section id="try-the-algorithm">
<h3>Try the algorithm<a class="headerlink" href="#try-the-algorithm" title="Permalink to this headline"></a></h3>
<p>Then we can run a training run to see how algorithms performs on your dataset</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.main train <span class="se">\</span>
        --dataset flat_MNIST <span class="se">\</span>
        --objective ERM <span class="se">\</span>
        --test_env <span class="m">0</span> <span class="se">\</span>
        --data_path ./data
</pre></div>
</div>
</section>
<section id="run-a-sweep">
<h3>Run a sweep<a class="headerlink" href="#run-a-sweep" title="Permalink to this headline"></a></h3>
<p>Finally, we can run a sweep to see how the algorithms performs on your dataset</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3 -m woods.scripts.hparams_sweep <span class="se">\</span>
        --objective ERM <span class="se">\</span>
        --dataset flat_MNIST <span class="se">\</span>
        --data_path ./data <span class="se">\</span>
        --launcher dummy
</pre></div>
</div>
</section>
</section>
<span id="document-contributing"></span><section id="contributing">
<h2>Contributing<a class="headerlink" href="#contributing" title="Permalink to this headline"></a></h2>
<p>Woods is still under developpement and is open to contributions. Just fork the repository and start coding! When you think you have something to contribute, open an issue or a pull request.</p>
<p>If you have a published algorithm that you want to be added as a benchmark please open a pull request we will be happy to add it to the list of available algorithms.</p>
<p>If you have a sequencial dataset that you think has a generalization problem, please open a pull request and we will be happy to add it to the list of available datasets.</p>
</section>
</div>
<section id="api-documentation">
<h2>API Documentation<a class="headerlink" href="#api-documentation" title="Permalink to this headline"></a></h2>
<div class="toctree-wrapper compound">
<span id="document-woods"></span><section id="module-woods">
<span id="woods"></span><h3>woods<a class="headerlink" href="#module-woods" title="Permalink to this headline"></a></h3>
<div class="toctree-wrapper compound">
<span id="document-woods.command_launchers"></span><section id="module-woods.command_launchers">
<span id="woods-command-launchers-module"></span><h4>woods.command_launchers module<a class="headerlink" href="#module-woods.command_launchers" title="Permalink to this headline"></a></h4>
<p>Set of functions used to launch lists of python scripts</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.command_launchers.dummy_launcher" title="woods.command_launchers.dummy_launcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dummy_launcher</span></code></a></p></td>
<td><p>Doesn't launch any scripts in commands, it only prints the commands.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.command_launchers.local_launcher" title="woods.command_launchers.local_launcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_launcher</span></code></a></p></td>
<td><p>Launch all of the scripts in commands on the local machine serially.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.command_launchers.slurm_launcher" title="woods.command_launchers.slurm_launcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slurm_launcher</span></code></a></p></td>
<td><p>Parallel job launcher for computationnal cluster using the SLURM workload manager.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.command_launchers.dummy_launcher">
<span class="sig-prename descclassname"><span class="pre">woods.command_launchers.</span></span><span class="sig-name descname"><span class="pre">dummy_launcher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">commands</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.command_launchers.dummy_launcher" title="Permalink to this definition"></a></dt>
<dd><p>Doesn’t launch any scripts in commands, it only prints the commands. Useful for testing.</p>
<p>Taken from : <a class="reference external" href="https://github.com/facebookresearch/DomainBed/">https://github.com/facebookresearch/DomainBed/</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>commands</strong> (<em>List</em>) – List of list of string that consists of a python script call</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.command_launchers.local_launcher">
<span class="sig-prename descclassname"><span class="pre">woods.command_launchers.</span></span><span class="sig-name descname"><span class="pre">local_launcher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">commands</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.command_launchers.local_launcher" title="Permalink to this definition"></a></dt>
<dd><p>Launch all of the scripts in commands on the local machine serially. If GPU is available it is gonna use it.</p>
<p>Taken from : <a class="reference external" href="https://github.com/facebookresearch/DomainBed/">https://github.com/facebookresearch/DomainBed/</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>commands</strong> (<em>List</em>) – List of list of string that consists of a python script call</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.command_launchers.slurm_launcher">
<span class="sig-prename descclassname"><span class="pre">woods.command_launchers.</span></span><span class="sig-name descname"><span class="pre">slurm_launcher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">commands</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.command_launchers.slurm_launcher" title="Permalink to this definition"></a></dt>
<dd><p>Parallel job launcher for computationnal cluster using the SLURM workload manager.</p>
<p>Launches all the jobs in commands in parallel according to the number of tasks in the slurm allocation.
An example of SBATCH options:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=&lt;job_name&gt;</span>
<span class="c1">#SBATCH --output=&lt;job_name&gt;.out</span>
<span class="c1">#SBATCH --error=&lt;job_name&gt;_error.out</span>
<span class="c1">#SBATCH --ntasks=4</span>
<span class="c1">#SBATCH --cpus-per-task=8</span>
<span class="c1">#SBATCH --gres=gpu:4</span>
<span class="c1">#SBATCH --time=1-00:00:00</span>
<span class="c1">#SBATCH --mem=81Gb</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>–cpus-per-task should match the N_WORKERS defined in datasets.py (default 4)</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>there should be equal number of –ntasks and –gres</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>commands</strong> (<em>List</em>) – List of list of string that consists of a python script call</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<span id="document-woods.datasets"></span><section id="module-woods.datasets">
<span id="woods-datasets-module"></span><h4>woods.datasets module<a class="headerlink" href="#module-woods.datasets" title="Permalink to this headline"></a></h4>
<p>Defining the benchmarks for OoD generalization in time-series</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Classes:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.Basic_Fourier" title="woods.datasets.Basic_Fourier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Basic_Fourier</span></code></a></p></td>
<td><p>Fourier_basic dataset</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.CAP" title="woods.datasets.CAP"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CAP</span></code></a></p></td>
<td><p>CAP Sleep stage dataset</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.EEG_DB" title="woods.datasets.EEG_DB"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EEG_DB</span></code></a></p></td>
<td><p>Class for Sleep Staging datasets with their data stored in a HDF5 file</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.H5_dataset" title="woods.datasets.H5_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">H5_dataset</span></code></a></p></td>
<td><p>HDF5 dataset for EEG data</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.HHAR" title="woods.datasets.HHAR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HHAR</span></code></a></p></td>
<td><p>Heterogeneity Acrivity Recognition Dataset (HHAR)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.InfiniteLoader" title="woods.datasets.InfiniteLoader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InfiniteLoader</span></code></a></p></td>
<td><p>InfiniteLoader is a torch.utils.data.IterableDataset that can be used to infinitely iterate over a finite dataset.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.InfiniteSampler" title="woods.datasets.InfiniteSampler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InfiniteSampler</span></code></a></p></td>
<td><p>Infinite Sampler for PyTorch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.LSA64" title="woods.datasets.LSA64"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSA64</span></code></a></p></td>
<td><p>LSA64: A Dataset for Argentinian Sign Language dataset</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Multi_Domain_Dataset</span></code></a></p></td>
<td><p>Abstract class of a multi domain dataset for OOD generalization.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.PCL" title="woods.datasets.PCL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PCL</span></code></a></p></td>
<td><p>PCL datasets</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.SEDFx" title="woods.datasets.SEDFx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SEDFx</span></code></a></p></td>
<td><p>SEDFx Sleep stage dataset</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.Spurious_Fourier" title="woods.datasets.Spurious_Fourier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Spurious_Fourier</span></code></a></p></td>
<td><p>Spurious_Fourier dataset</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.TCMNIST" title="woods.datasets.TCMNIST"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TCMNIST</span></code></a></p></td>
<td><p>Abstract class for Temporal Colored MNIST</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.TCMNIST_seq" title="woods.datasets.TCMNIST_seq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TCMNIST_seq</span></code></a></p></td>
<td><p>Temporal Colored MNIST Sequence</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.TCMNIST_step" title="woods.datasets.TCMNIST_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TCMNIST_step</span></code></a></p></td>
<td><p>Temporal Colored MNIST Step</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.TMNIST" title="woods.datasets.TMNIST"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TMNIST</span></code></a></p></td>
<td><p>Temporal MNIST dataset</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.Video_dataset" title="woods.datasets.Video_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Video_dataset</span></code></a></p></td>
<td><p>Video dataset</p></td>
</tr>
</tbody>
</table>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.XOR" title="woods.datasets.XOR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">XOR</span></code></a></p></td>
<td><p>Returns a XOR b (the 'Exclusive or' gate)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.bernoulli" title="woods.datasets.bernoulli"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bernoulli</span></code></a></p></td>
<td><p>Returns a tensor of 1.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.get_dataset_class" title="woods.datasets.get_dataset_class"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_dataset_class</span></code></a></p></td>
<td><p>Return the dataset class with the given name.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.get_environments" title="woods.datasets.get_environments"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_environments</span></code></a></p></td>
<td><p>Returns the environments of a dataset</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.get_setup" title="woods.datasets.get_setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_setup</span></code></a></p></td>
<td><p>Returns the setup of a dataset</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.get_split" title="woods.datasets.get_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_split</span></code></a></p></td>
<td><p>Generates the keys that are used to split a Torch TensorDataset into (1-holdout_fraction) / holdout_fraction.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.get_sweep_envs" title="woods.datasets.get_sweep_envs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_sweep_envs</span></code></a></p></td>
<td><p>Returns the list of test environments to investigate in the hyper parameter sweep</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.datasets.make_split" title="woods.datasets.make_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_split</span></code></a></p></td>
<td><p>Split a Torch TensorDataset into (1-holdout_fraction) / holdout_fraction.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.datasets.num_environments" title="woods.datasets.num_environments"><code class="xref py py-obj docutils literal notranslate"><span class="pre">num_environments</span></code></a></p></td>
<td><p>Returns the number of environments of a dataset</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.get_dataset_class">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">get_dataset_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.get_dataset_class" title="Permalink to this definition"></a></dt>
<dd><p>Return the dataset class with the given name.</p>
<p>Taken from : <a class="reference external" href="https://github.com/facebookresearch/DomainBed/">https://github.com/facebookresearch/DomainBed/</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset_name</strong> (<em>str</em>) – Name of the dataset to get the function of. (Must be a part of the DATASETS list)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The __init__ function of the desired dataset that takes as input (  flags: parser arguments of the train.py script, training_hparams: set of training hparams from hparams.py )</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>function</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – Dataset name not found in the datasets.py globals</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.num_environments">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">num_environments</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.num_environments" title="Permalink to this definition"></a></dt>
<dd><p>Returns the number of environments of a dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset_name</strong> (<em>str</em>) – Name of the dataset to get the number of environments of. (Must be a part of the DATASETS list)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Number of environments of the dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.get_sweep_envs">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">get_sweep_envs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.get_sweep_envs" title="Permalink to this definition"></a></dt>
<dd><p>Returns the list of test environments to investigate in the hyper parameter sweep</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset_name</strong> (<em>str</em>) – Name of the dataset to get the number of environments of. (Must be a part of the DATASETS list)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of environments to sweep across</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.get_environments">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">get_environments</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.get_environments" title="Permalink to this definition"></a></dt>
<dd><p>Returns the environments of a dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset_name</strong> (<em>str</em>) – Name of the dataset to get the number of environments of. (Must be a part of the DATASETS list)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of environments of the dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.get_setup">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">get_setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.get_setup" title="Permalink to this definition"></a></dt>
<dd><p>Returns the setup of a dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset_name</strong> (<em>str</em>) – Name of the dataset to get the number of environments of. (Must be a part of the DATASETS list)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The setup of the dataset (‘seq’ or ‘step’)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.XOR">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">XOR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.XOR" title="Permalink to this definition"></a></dt>
<dd><p>Returns a XOR b (the ‘Exclusive or’ gate)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<em>bool</em>) – First input</p></li>
<li><p><strong>b</strong> (<em>bool</em>) – Second input</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output of the XOR gate</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.bernoulli">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">bernoulli</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.bernoulli" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor of 1. (True) or 0. (False) resulting from the outcome of a bernoulli random variable of parameter p.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<em>float</em>) – Parameter p of the Bernoulli distribution</p></li>
<li><p><strong>size</strong> (<em>int</em><em>...</em>) – A sequence of integers defining hte shape of the output tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor of Bernoulli random variables of parameter p</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.make_split">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">make_split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">holdout_fraction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.make_split" title="Permalink to this definition"></a></dt>
<dd><p>Split a Torch TensorDataset into (1-holdout_fraction) / holdout_fraction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>TensorDataset</em>) – Tensor dataset that has 2 tensors -&gt; data, targets</p></li>
<li><p><strong>holdout_fraction</strong> (<em>float</em>) – Fraction of the dataset that is gonna be in the validation set</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em>) – seed used for the shuffling of the data before splitting. Defaults to 0.</p></li>
<li><p><strong>sort</strong> (<em>bool</em><em>, </em><em>optional</em>) – If ‘’True’’ the dataset is gonna be sorted after splitting. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1-holdout_fraction part of the split
TensorDataset: holdout_fractoin part of the split</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>TensorDataset</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.datasets.get_split">
<span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">get_split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">holdout_fraction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.get_split" title="Permalink to this definition"></a></dt>
<dd><p>Generates the keys that are used to split a Torch TensorDataset into (1-holdout_fraction) / holdout_fraction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>TensorDataset</em>) – TensorDataset to be split</p></li>
<li><p><strong>holdout_fraction</strong> (<em>float</em>) – Fraction of the dataset that is gonna be in the out (validation) set</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em>) – seed used for the shuffling of the data before splitting. Defaults to 0.</p></li>
<li><p><strong>sort</strong> (<em>bool</em><em>, </em><em>optional</em>) – If ‘’True’’ the dataset is gonna be sorted after splitting. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>in (1-holdout_fraction) keys of the split
list: out (holdout_fraction) keys of the split</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.InfiniteSampler">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">InfiniteSampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampler</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.InfiniteSampler" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.sampler.Sampler</span></code></p>
<p>Infinite Sampler for PyTorch.</p>
<p>Inspired from : <a class="reference external" href="https://github.com/facebookresearch/DomainBed">https://github.com/facebookresearch/DomainBed</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sampler</strong> (<em>torch.utils.data.Sampler</em>) – Sampler to be used for the infinite sampling.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.InfiniteLoader">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">InfiniteLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.InfiniteLoader" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.IterableDataset</span></code></p>
<p>InfiniteLoader is a torch.utils.data.IterableDataset that can be used to infinitely iterate over a finite dataset.</p>
<p>Inspired from : <a class="reference external" href="https://github.com/facebookresearch/DomainBed">https://github.com/facebookresearch/DomainBed</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be iterated over</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size of the dataset</p></li>
<li><p><strong>num_workers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of workers to use for the data loading. Defaults to 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">Multi_Domain_Dataset</span></span><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Abstract class of a multi domain dataset for OOD generalization.</p>
<p>Every multi domain dataset must redefine the important attributes: SETUP, PRED_TIME, ENVS, INPUT_SHAPE, OUTPUT_SIZE, TASK
The data dimension needs to be (batch_size, SEQ_LEN, <a href="#id1"><span class="problematic" id="id2">*</span></a>INPUT_SHAPE)</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5001</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.CHECKPOINT_FREQ">
<span class="sig-name descname"><span class="pre">CHECKPOINT_FREQ</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">100</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.CHECKPOINT_FREQ" title="Permalink to this definition"></a></dt>
<dd><p>The frequency of results update</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.N_WORKERS">
<span class="sig-name descname"><span class="pre">N_WORKERS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">4</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.N_WORKERS" title="Permalink to this definition"></a></dt>
<dd><p>The number of workers used for fast dataloaders used for validation</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[None]</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.DATA_PATH">
<span class="sig-name descname"><span class="pre">DATA_PATH</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.DATA_PATH" title="Permalink to this definition"></a></dt>
<dd><p>Path to the data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[None]</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[None]</span></em><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.loss_fn">
<span class="sig-name descname"><span class="pre">loss_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.loss_fn" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> (<em>Tensor</em>) – prediction tensor</p></li>
<li><p><strong>target</strong> (<em>Tensor</em>) – Target tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.get_class_weight">
<span class="sig-name descname"><span class="pre">get_class_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.get_class_weight" title="Permalink to this definition"></a></dt>
<dd><p>Compute class weight for class balanced training</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of weights of length OUTPUT_SIZE</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.get_train_loaders">
<span class="sig-name descname"><span class="pre">get_train_loaders</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.get_train_loaders" title="Permalink to this definition"></a></dt>
<dd><p>Fetch all training dataloaders and their ID</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of string names of the data splits used for training
list: list of dataloaders of the data splits used for training</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.get_val_loaders">
<span class="sig-name descname"><span class="pre">get_val_loaders</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.get_val_loaders" title="Permalink to this definition"></a></dt>
<dd><p>Fetch all validation/test dataloaders and their ID</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of string names of the data splits used for validation and test
list: list of dataloaders of the data splits used for validation and test</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.split_output">
<span class="sig-name descname"><span class="pre">split_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.split_output" title="Permalink to this definition"></a></dt>
<dd><p>Group data and prediction by environment</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>Tensor</em>) – output from a model of shape ((n_env-1)*batch_size, len(PRED_TIME), output_size)</p></li>
<li><p><strong>labels</strong> (<em>Tensor</em>) – labels of shape ((n_env-1)*batch_size, len(PRED_TIME), output_size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reshaped output (n_train_env, batch_size, len(PRED_TIME), output_size)
Tensor: The labels (n_train_env, batch_size, len(PRED_TIME))</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Multi_Domain_Dataset.split_labels">
<span class="sig-name descname"><span class="pre">split_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Multi_Domain_Dataset.split_labels" title="Permalink to this definition"></a></dt>
<dd><p>Group data and prediction by environment</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>Tensor</em>) – output from a model of shape ((n_env-1)*batch_size, len(PRED_TIME), output_size)</p></li>
<li><p><strong>labels</strong> (<em>Tensor</em>) – labels of shape ((n_env-1)*batch_size, len(PRED_TIME), output_size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reshaped output (n_train_env, batch_size, len(PRED_TIME), output_size)
Tensor: The labels (n_train_env, batch_size, len(PRED_TIME))</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">Basic_Fourier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Basic_Fourier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.Multi_Domain_Dataset</span></code></a></p>
<p>Fourier_basic dataset</p>
<p>A dataset of 1D sinusoid signal to classify according to their Fourier spectrum.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>No download is required as it is purely synthetic</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'seq'</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">50</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[49]</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[1]</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">2</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">['no_spur']</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Basic_Fourier.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[None]</span></em><a class="headerlink" href="#woods.datasets.Basic_Fourier.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">Spurious_Fourier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Spurious_Fourier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.Multi_Domain_Dataset</span></code></a></p>
<p>Spurious_Fourier dataset</p>
<p>A dataset of 1D sinusoid signal to classify according to their Fourier spectrum.
Peaks in the fourier spectrum are added to the signal that are spuriously correlated to the label.
Different environment have different correlation rates between the labels and the spurious peaks in the spectrum.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>No download is required as it is purely synthetic</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'seq'</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">50</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[49]</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[1]</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">2</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.LABEL_NOISE">
<span class="sig-name descname"><span class="pre">LABEL_NOISE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">0.25</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.LABEL_NOISE" title="Permalink to this definition"></a></dt>
<dd><p>Level of noise added to the labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0.1,</span> <span class="pre">0.8,</span> <span class="pre">0.9]</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The correlation rate between the label and the spurious peaks</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0]</span></em><a class="headerlink" href="#woods.datasets.Spurious_Fourier.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Spurious_Fourier.super_sample">
<span class="sig-name descname"><span class="pre">super_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">signal_0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">signal_1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Spurious_Fourier.super_sample" title="Permalink to this definition"></a></dt>
<dd><p>Sample signals frames with a bunch of offsets</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.TMNIST">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">TMNIST</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TMNIST" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.Multi_Domain_Dataset</span></code></a></p>
<p>Temporal MNIST dataset</p>
<p>Each sample is a sequence of 4 MNIST digits.
The task is to predict at each step if the sum of the current digit and the previous one is odd or even.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MNIST dataset needs to be downloaded, this is automaticaly done if the dataset isn’t in the given data_path</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5001</span></em><a class="headerlink" href="#woods.datasets.TMNIST.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'seq'</span></em><a class="headerlink" href="#woods.datasets.TMNIST.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.TMNIST.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">4</span></em><a class="headerlink" href="#woods.datasets.TMNIST.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span></em><a class="headerlink" href="#woods.datasets.TMNIST.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[1,</span> <span class="pre">28,</span> <span class="pre">28]</span></em><a class="headerlink" href="#woods.datasets.TMNIST.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">2</span></em><a class="headerlink" href="#woods.datasets.TMNIST.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">['grey']</span></em><a class="headerlink" href="#woods.datasets.TMNIST.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[None]</span></em><a class="headerlink" href="#woods.datasets.TMNIST.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.TMNIST.plot_samples">
<span class="sig-name descname"><span class="pre">plot_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">TMNIST_labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TMNIST.plot_samples" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">TCMNIST</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.Multi_Domain_Dataset</span></code></a></p>
<p>Abstract class for Temporal Colored MNIST</p>
<p>Each sample is a sequence of 4 MNIST digits.
The task is to predict at each step if the sum of the current digit and the previous one is odd or even.
Color is added to the digits that is correlated with the label of the current step.
The formulation of which is defined in the child of this class, either sequences-wise of step-wise</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MNIST dataset needs to be downloaded, this is automaticaly done if the dataset isn’t in the given data_path</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5001</span></em><a class="headerlink" href="#woods.datasets.TCMNIST.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.TCMNIST.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">4</span></em><a class="headerlink" href="#woods.datasets.TCMNIST.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span></em><a class="headerlink" href="#woods.datasets.TCMNIST.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[2,</span> <span class="pre">28,</span> <span class="pre">28]</span></em><a class="headerlink" href="#woods.datasets.TCMNIST.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">2</span></em><a class="headerlink" href="#woods.datasets.TCMNIST.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST.plot_samples">
<span class="sig-name descname"><span class="pre">plot_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">images</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST.plot_samples" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_seq">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">TCMNIST_seq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST_seq" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.TCMNIST" title="woods.datasets.TCMNIST"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.TCMNIST</span></code></a></p>
<p>Temporal Colored MNIST Sequence</p>
<p>Each sample is a sequence of 4 MNIST digits.
The task is to predict at each step if the sum of the current digit and the previous one is odd or even.
Color is added to the digits that is correlated with the label of the current step.</p>
<p>The correlation of the color to the label is constant across sequences and whole sequences are sampled from an environmnent definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MNIST dataset needs to be downloaded, this is automaticaly done if the dataset isn’t in the given data_path</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_seq.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'seq'</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_seq.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_seq.LABEL_NOISE">
<span class="sig-name descname"><span class="pre">LABEL_NOISE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">0.25</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_seq.LABEL_NOISE" title="Permalink to this definition"></a></dt>
<dd><p>Level of noise added to the labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_seq.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0.1,</span> <span class="pre">0.8,</span> <span class="pre">0.9]</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_seq.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>list of different correlation values between the color and the label</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_seq.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0]</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_seq.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_seq.color_dataset">
<span class="sig-name descname"><span class="pre">color_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">images</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST_seq.color_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Color the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>images</strong> (<em>Tensor</em>) – 3 channel images to color</p></li>
<li><p><strong>labels</strong> (<em>Tensor</em>) – labels of the images</p></li>
<li><p><strong>p</strong> (<em>float</em>) – correlation between the color and the label</p></li>
<li><p><strong>d</strong> (<em>float</em>) – level of noise added to the labels</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>colored images</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>colored_images (Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">TCMNIST_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST_step" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.TCMNIST" title="woods.datasets.TCMNIST"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.TCMNIST</span></code></a></p>
<p>Temporal Colored MNIST Step</p>
<p>Each sample is a sequence of 4 MNIST digits.
The task is to predict at each step if the sum of the current digit and the previous one is odd or even.
Color is added to the digits that is correlated with the label of the current step.</p>
<p>The correlation of the color to the label is varying across sequences and time steps are sampled from an environmnent definition.
By definition, the test environment is always the last time step in the sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MNIST dataset needs to be downloaded, this is automaticaly done if the dataset isn’t in the given data_path</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'step'</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_step.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step.LABEL_NOISE">
<span class="sig-name descname"><span class="pre">LABEL_NOISE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">0.25</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_step.LABEL_NOISE" title="Permalink to this definition"></a></dt>
<dd><p>Level of noise added to the labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0.9,</span> <span class="pre">0.8,</span> <span class="pre">0.1]</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_step.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>list of different correlation values between the color and the label</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[2]</span></em><a class="headerlink" href="#woods.datasets.TCMNIST_step.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step.color_dataset">
<span class="sig-name descname"><span class="pre">color_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">images</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST_step.color_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Color a single step ‘env_id’ of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>images</strong> (<em>Tensor</em>) – 3 channel images to color</p></li>
<li><p><strong>labels</strong> (<em>Tensor</em>) – labels of the images</p></li>
<li><p><strong>env_id</strong> (<em>int</em>) – environment id</p></li>
<li><p><strong>p</strong> (<em>float</em>) – correlation between the color and the label</p></li>
<li><p><strong>d</strong> (<em>float</em>) – level of noise added to the labels</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>all dataset with a new step colored</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>colored_images (Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step.split_output">
<span class="sig-name descname"><span class="pre">split_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST_step.split_output" title="Permalink to this definition"></a></dt>
<dd><p>Group data and prediction by environment</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>labels</strong> (<em>Tensor</em>) – labels of the data (batch_size, len(PRED_TIME))</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reshaped data (n_env-1, batch_size, 1, n_classes)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.TCMNIST_step.split_labels">
<span class="sig-name descname"><span class="pre">split_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.TCMNIST_step.split_labels" title="Permalink to this definition"></a></dt>
<dd><p>Group data and prediction by environment</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>labels</strong> (<em>Tensor</em>) – labels of the data (batch_size, len(PRED_TIME))</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reshaped labels (n_env-1, batch_size, 1)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.H5_dataset">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">H5_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">h5_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.H5_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>HDF5 dataset for EEG data</p>
<p>The HDF5 file is expected to have the following nested dict structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;env0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
          <span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">PRED_TIME</span><span class="p">))},</span>
<span class="s1">&#39;env1&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
         <span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">PRED_TIME</span><span class="p">))},</span>
<span class="o">...</span><span class="p">}</span>
</pre></div>
</div>
<p>Good thing about this is that it imports data only when it needs to and thus saves ram space</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>h5_path</strong> (<em>str</em>) – absolute path to the hdf5 file</p></li>
<li><p><strong>env_id</strong> (<em>int</em>) – environment id key in the hdf5 file</p></li>
<li><p><strong>split</strong> (<em>list</em>) – list of indices of the dataset the belong to the split. If ‘None’, all the data is used</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.H5_dataset.close">
<span class="sig-name descname"><span class="pre">close</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.H5_dataset.close" title="Permalink to this definition"></a></dt>
<dd><p>Close the hdf5 file link</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.EEG_DB">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">EEG_DB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.EEG_DB" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.Multi_Domain_Dataset</span></code></a></p>
<p>Class for Sleep Staging datasets with their data stored in a HDF5 file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.EEG_DB.CHECKPOINT_FREQ">
<span class="sig-name descname"><span class="pre">CHECKPOINT_FREQ</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">500</span></em><a class="headerlink" href="#woods.datasets.EEG_DB.CHECKPOINT_FREQ" title="Permalink to this definition"></a></dt>
<dd><p>The frequency of results update</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.EEG_DB.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'seq'</span></em><a class="headerlink" href="#woods.datasets.EEG_DB.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.EEG_DB.DATA_PATH">
<span class="sig-name descname"><span class="pre">DATA_PATH</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#woods.datasets.EEG_DB.DATA_PATH" title="Permalink to this definition"></a></dt>
<dd><p>realative path to the hdf5 file</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.EEG_DB.get_class_weight">
<span class="sig-name descname"><span class="pre">get_class_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.EEG_DB.get_class_weight" title="Permalink to this definition"></a></dt>
<dd><p>Compute class weight for class balanced training</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of weights of length OUTPUT_SIZE</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.CAP">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">CAP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.CAP" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.EEG_DB" title="woods.datasets.EEG_DB"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.EEG_DB</span></code></a></p>
<p>CAP Sleep stage dataset</p>
<p>The task is to classify the sleep stage from EEG and other modalities of signals.
This dataset only uses about half of the raw dataset because of the incompatibility of some measurements.
We use the 5 most commonly used machines in the database to create the 5 seperate environment to train on.
The machines that were used were infered by grouping together the recording that had the same channels, and the
final preprocessed data only include the channels that were in common between those 5 machines.</p>
<p>You can read more on the data itself and it’s provenance on Physionet.org:</p>
<blockquote>
<div><p><a class="reference external" href="https://physionet.org/content/capslpdb/1.0.0/">https://physionet.org/content/capslpdb/1.0.0/</a></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This dataset need to be downloaded and preprocessed. This can be done with the download.py script.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5001</span></em><a class="headerlink" href="#woods.datasets.CAP.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.CAP.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">3000</span></em><a class="headerlink" href="#woods.datasets.CAP.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[2999]</span></em><a class="headerlink" href="#woods.datasets.CAP.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[19]</span></em><a class="headerlink" href="#woods.datasets.CAP.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">6</span></em><a class="headerlink" href="#woods.datasets.CAP.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.DATA_PATH">
<span class="sig-name descname"><span class="pre">DATA_PATH</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'CAP/CAP.h5'</span></em><a class="headerlink" href="#woods.datasets.CAP.DATA_PATH" title="Permalink to this definition"></a></dt>
<dd><p>realative path to the hdf5 file</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">['Machine0',</span> <span class="pre">'Machine1',</span> <span class="pre">'Machine2',</span> <span class="pre">'Machine3',</span> <span class="pre">'Machine4']</span></em><a class="headerlink" href="#woods.datasets.CAP.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.CAP.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4]</span></em><a class="headerlink" href="#woods.datasets.CAP.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.SEDFx">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">SEDFx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.SEDFx" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.EEG_DB" title="woods.datasets.EEG_DB"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.EEG_DB</span></code></a></p>
<p>SEDFx Sleep stage dataset</p>
<p>The task is to classify the sleep stage from EEG and other modalities of signals.
This dataset only uses about half of the raw dataset because of the incompatibility of some measurements.
We split the dataset in 5 environments to train on, each of them containing the data taken from a given group age.</p>
<p>You can read more on the data itself and it’s provenance on Physionet.org:</p>
<blockquote>
<div><p><a class="reference external" href="https://physionet.org/content/sleep-edfx/1.0.0/">https://physionet.org/content/sleep-edfx/1.0.0/</a></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This dataset need to be downloaded and preprocessed. This can be done with the download.py script</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">10001</span></em><a class="headerlink" href="#woods.datasets.SEDFx.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.SEDFx.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">3000</span></em><a class="headerlink" href="#woods.datasets.SEDFx.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[2999]</span></em><a class="headerlink" href="#woods.datasets.SEDFx.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[4]</span></em><a class="headerlink" href="#woods.datasets.SEDFx.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">6</span></em><a class="headerlink" href="#woods.datasets.SEDFx.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.DATA_PATH">
<span class="sig-name descname"><span class="pre">DATA_PATH</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'SEDFx/SEDFx.h5'</span></em><a class="headerlink" href="#woods.datasets.SEDFx.DATA_PATH" title="Permalink to this definition"></a></dt>
<dd><p>realative path to the hdf5 file</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">['Age</span> <span class="pre">20-40',</span> <span class="pre">'Age</span> <span class="pre">40-60',</span> <span class="pre">'Age</span> <span class="pre">60-80',</span> <span class="pre">'Age</span> <span class="pre">80-100']</span></em><a class="headerlink" href="#woods.datasets.SEDFx.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.SEDFx.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3]</span></em><a class="headerlink" href="#woods.datasets.SEDFx.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.PCL">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">PCL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.PCL" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.EEG_DB" title="woods.datasets.EEG_DB"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.EEG_DB</span></code></a></p>
<p>PCL datasets</p>
<p>The task is to classify the motor imaginary from EEG and other modalities of signals.
The raw data comes from the three PCL Databases:</p>
<blockquote>
<div><p>[ ‘PhysionetMI’, ‘Cho2017’, ‘Lee2019_MI’]</p>
</div></blockquote>
<p>You can read more on the data itself and it’s provenance on:</p>
<blockquote>
<div><p><a class="reference external" href="http://moabb.neurotechx.com/docs/index.html">http://moabb.neurotechx.com/docs/index.html</a></p>
</div></blockquote>
<p>This dataset need to be downloaded and preprocessed. This can be done with the download.py script</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">10001</span></em><a class="headerlink" href="#woods.datasets.PCL.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.PCL.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">752</span></em><a class="headerlink" href="#woods.datasets.PCL.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[751]</span></em><a class="headerlink" href="#woods.datasets.PCL.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[48]</span></em><a class="headerlink" href="#woods.datasets.PCL.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">2</span></em><a class="headerlink" href="#woods.datasets.PCL.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.DATA_PATH">
<span class="sig-name descname"><span class="pre">DATA_PATH</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'PCL/PCL.h5'</span></em><a class="headerlink" href="#woods.datasets.PCL.DATA_PATH" title="Permalink to this definition"></a></dt>
<dd><p>realative path to the hdf5 file</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">['PhysionetMI',</span> <span class="pre">'Cho2017',</span> <span class="pre">'Lee2019_MI']</span></em><a class="headerlink" href="#woods.datasets.PCL.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.PCL.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2]</span></em><a class="headerlink" href="#woods.datasets.PCL.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.Video_dataset">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">Video_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_paths</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_frames</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Video_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>Video dataset</p>
<p>Folder structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>data_path
    └── 001
        └─ 001
            ├── frame000001.jpg
            ├── ...
            └── frame0000{n_frames}.jpg
        └─ 002
        └─ (samples) ...
    └── 002
        └─ 001
        └─ 002
        └─ (samples) ...
    └── 003
    └── (labels) ...
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_path</strong> (<em>str</em>) – path to the folder containing the data</p></li>
<li><p><strong>n_frames</strong> (<em>int</em>) – number of frames in each video</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – Optional transform to be applied
on a sample.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.Video_dataset.read_images">
<span class="sig-name descname"><span class="pre">read_images</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">selected_folder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_transform</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.Video_dataset.read_images" title="Permalink to this definition"></a></dt>
<dd><p>Read images from a folder (single video consisting of n_frames images)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>selected_folder</strong> (<em>str</em>) – path to the folder containing the images</p></li>
<li><p><strong>use_transform</strong> (<em>callable</em>) – transform to apply on the images</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>images tensor (n_frames, 3, 224, 224)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.LSA64">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">LSA64</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.LSA64" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.Multi_Domain_Dataset</span></code></a></p>
<p>LSA64: A Dataset for Argentinian Sign Language dataset</p>
<p>This dataset is composed of videos of different signers.</p>
<p>You can read more on the data itself and it’s provenance from it’s source:</p>
<blockquote>
<div><p><a class="reference external" href="http://facundoq.github.io/datasets/lsa64/">http://facundoq.github.io/datasets/lsa64/</a></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This dataset need to be downloaded and preprocessed. This can be done with the download.py script</p>
</div>
<dl class="simple">
<dt>Ressources:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="http://facundoq.github.io/datasets/lsa64/">http://facundoq.github.io/datasets/lsa64/</a></p></li>
<li><p><a class="reference external" href="http://facundoq.github.io/guides/sign_language_datasets/slr">http://facundoq.github.io/guides/sign_language_datasets/slr</a></p></li>
<li><p><a class="reference external" href="https://sci-hub.mksa.top/10.1007/978-981-10-7566-7_63">https://sci-hub.mksa.top/10.1007/978-981-10-7566-7_63</a></p></li>
<li><p><a class="reference external" href="https://github.com/hthuwal/sign-language-gesture-recognition/">https://github.com/hthuwal/sign-language-gesture-recognition/</a></p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5001</span></em><a class="headerlink" href="#woods.datasets.LSA64.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.CHECKPOINT_FREQ">
<span class="sig-name descname"><span class="pre">CHECKPOINT_FREQ</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">500</span></em><a class="headerlink" href="#woods.datasets.LSA64.CHECKPOINT_FREQ" title="Permalink to this definition"></a></dt>
<dd><p>The frequency of results update</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'seq'</span></em><a class="headerlink" href="#woods.datasets.LSA64.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.LSA64.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">20</span></em><a class="headerlink" href="#woods.datasets.LSA64.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>number of frames in each video</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[19]</span></em><a class="headerlink" href="#woods.datasets.LSA64.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[3,</span> <span class="pre">224,</span> <span class="pre">224]</span></em><a class="headerlink" href="#woods.datasets.LSA64.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">64</span></em><a class="headerlink" href="#woods.datasets.LSA64.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.DATA_PATH">
<span class="sig-name descname"><span class="pre">DATA_PATH</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'LSA64'</span></em><a class="headerlink" href="#woods.datasets.LSA64.DATA_PATH" title="Permalink to this definition"></a></dt>
<dd><p>path to the folder containing the data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">['001-002',</span> <span class="pre">'003-004',</span> <span class="pre">'005-006',</span> <span class="pre">'007-008',</span> <span class="pre">'009-010']</span></em><a class="headerlink" href="#woods.datasets.LSA64.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.LSA64.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4]</span></em><a class="headerlink" href="#woods.datasets.LSA64.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.datasets.LSA64.get_class_weight">
<span class="sig-name descname"><span class="pre">get_class_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.LSA64.get_class_weight" title="Permalink to this definition"></a></dt>
<dd><p>Compute class weight for class balanced training</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of weights of length OUTPUT_SIZE</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.datasets.HHAR">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.datasets.</span></span><span class="sig-name descname"><span class="pre">HHAR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.datasets.HHAR" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.datasets.Multi_Domain_Dataset</span></code></a></p>
<p>Heterogeneity Acrivity Recognition Dataset (HHAR)</p>
<p>This dataset is composed of wearables measurements during different activities.
The goal is to classify those activities (stand, sit, walk, bike, stairs up, stairs down).</p>
<p>You can read more on the data itself and it’s provenance from it’s source:</p>
<blockquote>
<div><p><a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition">https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition</a></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>argparse.Namespace</em>) – argparse of training arguments</p></li>
<li><p><strong>training_hparams</strong> (<em>dict</em>) – dictionnary of training hyper parameters coming from the hyperparams.py file</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This dataset need to be downloaded and preprocessed. This can be done with the download.py script</p>
</div>
<dl class="simple">
<dt>Ressources:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition">https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/10.1145/2809695.2809718">https://dl.acm.org/doi/10.1145/2809695.2809718</a></p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.N_STEPS">
<span class="sig-name descname"><span class="pre">N_STEPS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5001</span></em><a class="headerlink" href="#woods.datasets.HHAR.N_STEPS" title="Permalink to this definition"></a></dt>
<dd><p>The number of training steps taken for this dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.CHECKPOINT_FREQ">
<span class="sig-name descname"><span class="pre">CHECKPOINT_FREQ</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">100</span></em><a class="headerlink" href="#woods.datasets.HHAR.CHECKPOINT_FREQ" title="Permalink to this definition"></a></dt>
<dd><p>The frequency of results update</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.SETUP">
<span class="sig-name descname"><span class="pre">SETUP</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'seq'</span></em><a class="headerlink" href="#woods.datasets.HHAR.SETUP" title="Permalink to this definition"></a></dt>
<dd><p>The setup of the dataset (‘seq’ or ‘step’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.TASK">
<span class="sig-name descname"><span class="pre">TASK</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'classification'</span></em><a class="headerlink" href="#woods.datasets.HHAR.TASK" title="Permalink to this definition"></a></dt>
<dd><p>The type of prediction task (‘classification’ of ‘regression’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.SEQ_LEN">
<span class="sig-name descname"><span class="pre">SEQ_LEN</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">500</span></em><a class="headerlink" href="#woods.datasets.HHAR.SEQ_LEN" title="Permalink to this definition"></a></dt>
<dd><p>The sequence length of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.PRED_TIME">
<span class="sig-name descname"><span class="pre">PRED_TIME</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[499]</span></em><a class="headerlink" href="#woods.datasets.HHAR.PRED_TIME" title="Permalink to this definition"></a></dt>
<dd><p>The time steps where predictions are made</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.INPUT_SHAPE">
<span class="sig-name descname"><span class="pre">INPUT_SHAPE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[6]</span></em><a class="headerlink" href="#woods.datasets.HHAR.INPUT_SHAPE" title="Permalink to this definition"></a></dt>
<dd><p>The shape of the input (excluding batch size and time dimension)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.OUTPUT_SIZE">
<span class="sig-name descname"><span class="pre">OUTPUT_SIZE</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">6</span></em><a class="headerlink" href="#woods.datasets.HHAR.OUTPUT_SIZE" title="Permalink to this definition"></a></dt>
<dd><p>The size of the output</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.DATA_PATH">
<span class="sig-name descname"><span class="pre">DATA_PATH</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'HHAR/HHAR.h5'</span></em><a class="headerlink" href="#woods.datasets.HHAR.DATA_PATH" title="Permalink to this definition"></a></dt>
<dd><p>Path to the file containing the data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.ENVS">
<span class="sig-name descname"><span class="pre">ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">['nexus4',</span> <span class="pre">'s3',</span> <span class="pre">'s3mini',</span> <span class="pre">'lgwatch',</span> <span class="pre">'gear']</span></em><a class="headerlink" href="#woods.datasets.HHAR.ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments of the dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.datasets.HHAR.SWEEP_ENVS">
<span class="sig-name descname"><span class="pre">SWEEP_ENVS</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4]</span></em><a class="headerlink" href="#woods.datasets.HHAR.SWEEP_ENVS" title="Permalink to this definition"></a></dt>
<dd><p>The environments that should be used for testing (One at a time). These will be the test environments used in the sweeps</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<span id="document-woods.hyperparams"></span><section id="module-woods.hyperparams">
<span id="woods-hyperparams-module"></span><h4>woods.hyperparams module<a class="headerlink" href="#module-woods.hyperparams" title="Permalink to this headline"></a></h4>
<p>Defining hyper parameters and their distributions for HPO</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.ANDMask_hyper" title="woods.hyperparams.ANDMask_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ANDMask_hyper</span></code></a></p></td>
<td><p>ANDMask objective hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.Basic_Fourier_model" title="woods.hyperparams.Basic_Fourier_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Basic_Fourier_model</span></code></a></p></td>
<td><p>Spurious Fourier model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.Basic_Fourier_train" title="woods.hyperparams.Basic_Fourier_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Basic_Fourier_train</span></code></a></p></td>
<td><p>Basic Fourier model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.CAP_model" title="woods.hyperparams.CAP_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CAP_model</span></code></a></p></td>
<td><p>CAP model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.CAP_train" title="woods.hyperparams.CAP_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CAP_train</span></code></a></p></td>
<td><p>CAP model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.ERM_hyper" title="woods.hyperparams.ERM_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ERM_hyper</span></code></a></p></td>
<td><p>ERM objective hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.Fish_hyper" title="woods.hyperparams.Fish_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Fish_hyper</span></code></a></p></td>
<td><p>Fish objective hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.IGA_hyper" title="woods.hyperparams.IGA_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IGA_hyper</span></code></a></p></td>
<td><p>IGA objective hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.IRM_hyper" title="woods.hyperparams.IRM_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IRM_hyper</span></code></a></p></td>
<td><p>IRM objective hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.LSA64_model" title="woods.hyperparams.LSA64_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSA64_model</span></code></a></p></td>
<td><p>LSA64 model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.LSA64_train" title="woods.hyperparams.LSA64_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSA64_train</span></code></a></p></td>
<td><p>LSA64 model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.SANDMask_hyper" title="woods.hyperparams.SANDMask_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SANDMask_hyper</span></code></a></p></td>
<td><p>SANDMask objective hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.SD_hyper" title="woods.hyperparams.SD_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SD_hyper</span></code></a></p></td>
<td><p>SD objective hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.SEDFx_model" title="woods.hyperparams.SEDFx_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SEDFx_model</span></code></a></p></td>
<td><p>SEDFx model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.SEDFx_train" title="woods.hyperparams.SEDFx_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SEDFx_train</span></code></a></p></td>
<td><p>SEDFx model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.Spurious_Fourier_model" title="woods.hyperparams.Spurious_Fourier_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Spurious_Fourier_model</span></code></a></p></td>
<td><p>Spurious Fourier model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.Spurious_Fourier_train" title="woods.hyperparams.Spurious_Fourier_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Spurious_Fourier_train</span></code></a></p></td>
<td><p>Spurious Fourier model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.TCMNIST_seq_model" title="woods.hyperparams.TCMNIST_seq_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TCMNIST_seq_model</span></code></a></p></td>
<td><p>TCMNIST_seq model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.TCMNIST_seq_train" title="woods.hyperparams.TCMNIST_seq_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TCMNIST_seq_train</span></code></a></p></td>
<td><p>TCMNIST_seq model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.TCMNIST_step_model" title="woods.hyperparams.TCMNIST_step_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TCMNIST_step_model</span></code></a></p></td>
<td><p>TCMNIST_step model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.TCMNIST_step_train" title="woods.hyperparams.TCMNIST_step_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TCMNIST_step_train</span></code></a></p></td>
<td><p>TCMNIST_step model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.TMNIST_model" title="woods.hyperparams.TMNIST_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TMNIST_model</span></code></a></p></td>
<td><p>TMNIST model hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.TMNIST_train" title="woods.hyperparams.TMNIST_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TMNIST_train</span></code></a></p></td>
<td><p>TMNIST model hparam definition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.VREx_hyper" title="woods.hyperparams.VREx_hyper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VREx_hyper</span></code></a></p></td>
<td><p>VREx objective hparam definition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.get_model_hparams" title="woods.hyperparams.get_model_hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model_hparams</span></code></a></p></td>
<td><p>Get the model related hyper parameters</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.hyperparams.get_objective_hparams" title="woods.hyperparams.get_objective_hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_objective_hparams</span></code></a></p></td>
<td><p>Get the objective related hyper parameters</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.hyperparams.get_training_hparams" title="woods.hyperparams.get_training_hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_training_hparams</span></code></a></p></td>
<td><p>Get training related hyper parameters (class_balance, weight_decay, lr, batch_size)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.get_training_hparams">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">get_training_hparams</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.get_training_hparams" title="Permalink to this definition"></a></dt>
<dd><p>Get training related hyper parameters (class_balance, weight_decay, lr, batch_size)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_name</strong> (<em>str</em>) – dataset that is gonna be trained on for the run</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – seed used if hyper parameter is sampled</p></li>
<li><p><strong>sample</strong> (<em>bool</em><em>, </em><em>optional</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – Dataset name not found</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionnary with hyper parameters values</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.Basic_Fourier_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">Basic_Fourier_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.Basic_Fourier_train" title="Permalink to this definition"></a></dt>
<dd><p>Basic Fourier model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.Spurious_Fourier_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">Spurious_Fourier_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.Spurious_Fourier_train" title="Permalink to this definition"></a></dt>
<dd><p>Spurious Fourier model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.TMNIST_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">TMNIST_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.TMNIST_train" title="Permalink to this definition"></a></dt>
<dd><p>TMNIST model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.TCMNIST_seq_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">TCMNIST_seq_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.TCMNIST_seq_train" title="Permalink to this definition"></a></dt>
<dd><p>TCMNIST_seq model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.TCMNIST_step_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">TCMNIST_step_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.TCMNIST_step_train" title="Permalink to this definition"></a></dt>
<dd><p>TCMNIST_step model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.CAP_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">CAP_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.CAP_train" title="Permalink to this definition"></a></dt>
<dd><p>CAP model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.SEDFx_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">SEDFx_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.SEDFx_train" title="Permalink to this definition"></a></dt>
<dd><p>SEDFx model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.LSA64_train">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">LSA64_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.LSA64_train" title="Permalink to this definition"></a></dt>
<dd><p>LSA64 model hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.get_model_hparams">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">get_model_hparams</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.get_model_hparams" title="Permalink to this definition"></a></dt>
<dd><p>Get the model related hyper parameters</p>
<p>Each dataset has their own model hyper parameters definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_name</strong> (<em>str</em>) – dataset that is gonna be trained on for the run</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – seed used if hyper parameter is sampled</p></li>
<li><p><strong>sample</strong> (<em>bool</em><em>, </em><em>optional</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – Dataset name not found</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionnary with hyper parameters values</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.Basic_Fourier_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">Basic_Fourier_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.Basic_Fourier_model" title="Permalink to this definition"></a></dt>
<dd><p>Spurious Fourier model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.Spurious_Fourier_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">Spurious_Fourier_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.Spurious_Fourier_model" title="Permalink to this definition"></a></dt>
<dd><p>Spurious Fourier model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.TMNIST_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">TMNIST_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.TMNIST_model" title="Permalink to this definition"></a></dt>
<dd><p>TMNIST model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.TCMNIST_seq_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">TCMNIST_seq_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.TCMNIST_seq_model" title="Permalink to this definition"></a></dt>
<dd><p>TCMNIST_seq model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.TCMNIST_step_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">TCMNIST_step_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.TCMNIST_step_model" title="Permalink to this definition"></a></dt>
<dd><p>TCMNIST_step model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.CAP_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">CAP_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.CAP_model" title="Permalink to this definition"></a></dt>
<dd><p>CAP model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.SEDFx_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">SEDFx_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.SEDFx_model" title="Permalink to this definition"></a></dt>
<dd><p>SEDFx model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.LSA64_model">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">LSA64_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.LSA64_model" title="Permalink to this definition"></a></dt>
<dd><p>LSA64 model hparam definition</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.get_objective_hparams">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">get_objective_hparams</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objective_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.get_objective_hparams" title="Permalink to this definition"></a></dt>
<dd><p>Get the objective related hyper parameters</p>
<p>Each objective has their own model hyper parameters definitions</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>objective_name</strong> (<em>str</em>) – objective that is gonna be trained on for the run</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – seed used if hyper parameter is sampled</p></li>
<li><p><strong>sample</strong> (<em>bool</em><em>, </em><em>optional</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – Objective name not found</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionnary with hyper parameters values</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.ERM_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">ERM_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.ERM_hyper" title="Permalink to this definition"></a></dt>
<dd><p>ERM objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.IRM_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">IRM_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.IRM_hyper" title="Permalink to this definition"></a></dt>
<dd><p>IRM objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.VREx_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">VREx_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.VREx_hyper" title="Permalink to this definition"></a></dt>
<dd><p>VREx objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.SD_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">SD_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.SD_hyper" title="Permalink to this definition"></a></dt>
<dd><p>SD objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.IGA_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">IGA_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.IGA_hyper" title="Permalink to this definition"></a></dt>
<dd><p>IGA objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.ANDMask_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">ANDMask_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.ANDMask_hyper" title="Permalink to this definition"></a></dt>
<dd><p>ANDMask objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.Fish_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">Fish_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.Fish_hyper" title="Permalink to this definition"></a></dt>
<dd><p>Fish objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.hyperparams.SANDMask_hyper">
<span class="sig-prename descclassname"><span class="pre">woods.hyperparams.</span></span><span class="sig-name descname"><span class="pre">SANDMask_hyper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.hyperparams.SANDMask_hyper" title="Permalink to this definition"></a></dt>
<dd><p>SANDMask objective hparam definition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample</strong> (<em>bool</em>) – If ‘’True’’, hyper parameters are gonna be sampled randomly according to their given distributions. Defaults to ‘’False’’ where the default value is chosen.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<span id="document-woods.model_selection"></span><section id="module-woods.model_selection">
<span id="woods-model-selection-module"></span><h4>woods.model_selection module<a class="headerlink" href="#module-woods.model_selection" title="Permalink to this headline"></a></h4>
<p>Defining the model selection strategies</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.model_selection.IID_validation" title="woods.model_selection.IID_validation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IID_validation</span></code></a></p></td>
<td><p>Perform the IID validation model section on a single training run with NO TEST ENVIRONMENT and returns the results</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.model_selection.ensure_dict_path" title="woods.model_selection.ensure_dict_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ensure_dict_path</span></code></a></p></td>
<td><p>Ensure that a path of a nested dictionnary exists.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.model_selection.get_best_hparams" title="woods.model_selection.get_best_hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_best_hparams</span></code></a></p></td>
<td><p>Get the best set of hyperparameters for a given a record from a sweep and a selection method</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.model_selection.get_chosen_test_acc" title="woods.model_selection.get_chosen_test_acc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_chosen_test_acc</span></code></a></p></td>
<td><p>Get the test accuracy that will be chosen through the selection method for a given a record from a sweep</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.model_selection.test_domain_validation" title="woods.model_selection.test_domain_validation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_domain_validation</span></code></a></p></td>
<td><p>Perform the test domain validation model section on a single training run and returns the results</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.model_selection.train_domain_validation" title="woods.model_selection.train_domain_validation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_domain_validation</span></code></a></p></td>
<td><p>Perform the train domain validation model section on a single training run and returns the results</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.model_selection.ensure_dict_path">
<span class="sig-prename descclassname"><span class="pre">woods.model_selection.</span></span><span class="sig-name descname"><span class="pre">ensure_dict_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.model_selection.ensure_dict_path" title="Permalink to this definition"></a></dt>
<dd><p>Ensure that a path of a nested dictionnary exists.</p>
<p>If it does, return the nested dictionnary within. If it does not, create a nested dictionnary and return it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dict</strong> (<em>dict</em>) – Nested dictionnary to ensure a path</p></li>
<li><p><strong>key</strong> (<em>str</em>) – Key to ensure has a dictionnary in</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>nested dictionnary</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.model_selection.get_best_hparams">
<span class="sig-prename descclassname"><span class="pre">woods.model_selection.</span></span><span class="sig-name descname"><span class="pre">get_best_hparams</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">records</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection_method</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.model_selection.get_best_hparams" title="Permalink to this definition"></a></dt>
<dd><p>Get the best set of hyperparameters for a given a record from a sweep and a selection method</p>
<p>The way model selection is performed is by computing the validation accuracy of all training checkpoints.
The definition of the validation accuracy is given by the selection method.
Then using these validation accuracies, we choose the best checkpoint and report the corresponding hyperparameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>records</strong> (<em>dict</em>) – Dictionary of records from a sweep</p></li>
<li><p><strong>selection_method</strong> (<em>str</em>) – Selection method to use</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>flags of the chosen model training run for the all trial seeds
dict: hyperparameters of the chosen model for all trial seeds
dict: validation accuracy of the chosen model run for all trial seeds
dict: test accuracy of the chosen model run for all trial seeds</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.model_selection.get_chosen_test_acc">
<span class="sig-prename descclassname"><span class="pre">woods.model_selection.</span></span><span class="sig-name descname"><span class="pre">get_chosen_test_acc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">records</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection_method</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.model_selection.get_chosen_test_acc" title="Permalink to this definition"></a></dt>
<dd><p>Get the test accuracy that will be chosen through the selection method for a given a record from a sweep</p>
<p>The way model selection is performed is by computing the validation accuracy of all training checkpoints.
The definition of the validation accuracy is given by the selection method.
Then using these validation accuracies, we choose the best checkpoint and report the test accuracy linked to that checkpoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>records</strong> (<em>dict</em>) – Dictionary of records from a sweep</p></li>
<li><p><strong>selection_method</strong> (<em>str</em>) – Selection method to use</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>validation accuracy of the chosen models averaged over all trial seeds
float: variance of the validation accuracy of the chosen models accross all trial seeds
float: test accuracy of the chosen models averaged over all trial seeds
float: variance of the test accuracy of the chosen models accross all trial seeds</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.model_selection.IID_validation">
<span class="sig-prename descclassname"><span class="pre">woods.model_selection.</span></span><span class="sig-name descname"><span class="pre">IID_validation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">records</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.model_selection.IID_validation" title="Permalink to this definition"></a></dt>
<dd><p>Perform the IID validation model section on a single training run with NO TEST ENVIRONMENT and returns the results</p>
<dl class="simple">
<dt>The model selection is performed by computing the average all domains accuracy of all training checkpoints and choosing the highest one.</dt><dd><p>best_step = argmax_{step in checkpoints}( mean(train_envs_acc) )</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>records</strong> (<em>dict</em>) – Dictionary of records from a single training run</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>validation accuracy of the best checkpoint of the training run
float: validation accuracy of the best checkpoint of the training run</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is for ONLY for sweeps with no test environments.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.model_selection.train_domain_validation">
<span class="sig-prename descclassname"><span class="pre">woods.model_selection.</span></span><span class="sig-name descname"><span class="pre">train_domain_validation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">records</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.model_selection.train_domain_validation" title="Permalink to this definition"></a></dt>
<dd><p>Perform the train domain validation model section on a single training run and returns the results</p>
<dl class="simple">
<dt>The model selection is performed by computing the average training domains accuracy of all training checkpoints and choosing the highest one.</dt><dd><p>best_step = argmax_{step in checkpoints}( mean(train_envs_acc) )</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>records</strong> (<em>dict</em>) – Dictionary of records from a single training run</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>validation accuracy of the best checkpoint of the training run
float: test accuracy of the best checkpoint (highest validation accuracy) of the training run</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.model_selection.test_domain_validation">
<span class="sig-prename descclassname"><span class="pre">woods.model_selection.</span></span><span class="sig-name descname"><span class="pre">test_domain_validation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">records</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.model_selection.test_domain_validation" title="Permalink to this definition"></a></dt>
<dd><p>Perform the test domain validation model section on a single training run and returns the results</p>
<dl class="simple">
<dt>The model selection is performed with the test accuracy of ONLY THE LAST CHECKPOINT OF A TRAINING RUN, so this function simply returns the test accuracy of the last checkpoint.</dt><dd><p>best_step = test_acc[-1]</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>records</strong> (<em>dict</em>) – Dictionary of records from a single training run</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>validation accuracy of the training run, which is also the test accuracyof the last checkpoint
float: test accuracy of the last checkpoint</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<span id="document-woods.models"></span><section id="module-woods.models">
<span id="woods-models-module"></span><h4>woods.models module<a class="headerlink" href="#module-woods.models" title="Permalink to this headline"></a></h4>
<p>Defining the architectures used for benchmarking algorithms</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Classes:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.models.ATTN_LSTM" title="woods.models.ATTN_LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ATTN_LSTM</span></code></a></p></td>
<td><p>A simple LSTM model with self attention</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.models.CRNN" title="woods.models.CRNN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CRNN</span></code></a></p></td>
<td><p>Convolutional Recurrent Neural Network</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.models.EEGNet" title="woods.models.EEGNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EEGNet</span></code></a></p></td>
<td><p>The EEGNet model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.models.LSTM" title="woods.models.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTM</span></code></a></p></td>
<td><p>A simple LSTM model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.models.MNIST_CNN" title="woods.models.MNIST_CNN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MNIST_CNN</span></code></a></p></td>
<td><p>Hand-tuned architecture for extracting representation from MNIST images</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.models.MNIST_LSTM" title="woods.models.MNIST_LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MNIST_LSTM</span></code></a></p></td>
<td><p>A simple LSTM model taking inputs from a CNN.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.models.deep4" title="woods.models.deep4"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deep4</span></code></a></p></td>
<td><p>The DEEP4 model</p></td>
</tr>
</tbody>
</table>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.models.get_model" title="woods.models.get_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model</span></code></a></p></td>
<td><p>Return the dataset class with the given name</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.models.get_model">
<span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">get_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.get_model" title="Permalink to this definition"></a></dt>
<dd><p>Return the dataset class with the given name</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>str</em>) – name of the dataset</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – model hyperparameters</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.models.deep4">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">deep4</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.deep4" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The DEEP4 model</p>
<dl class="simple">
<dt>This is from the Braindecode package:</dt><dd><p><a class="reference external" href="https://github.com/braindecode/braindecode">https://github.com/braindecode/braindecode</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – dataset that we will be training on</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – The hyperparameters for the model.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.deep4.input_size">
<span class="sig-name descname"><span class="pre">input_size</span></span><a class="headerlink" href="#woods.models.deep4.input_size" title="Permalink to this definition"></a></dt>
<dd><p>The size of the inputs to the model (for a single time step).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.deep4.output_size">
<span class="sig-name descname"><span class="pre">output_size</span></span><a class="headerlink" href="#woods.models.deep4.output_size" title="Permalink to this definition"></a></dt>
<dd><p>The size of the outputs of the model (number of classes).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.deep4.seq_len">
<span class="sig-name descname"><span class="pre">seq_len</span></span><a class="headerlink" href="#woods.models.deep4.seq_len" title="Permalink to this definition"></a></dt>
<dd><p>The length of the sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.models.deep4.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.deep4.forward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.deep4.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.models.deep4.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.models.EEGNet">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">EEGNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.EEGNet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The EEGNet model</p>
<p>This is a really small model ~3k parameters.</p>
<dl class="simple">
<dt>This is from the Braindecode package:</dt><dd><p><a class="reference external" href="https://github.com/braindecode/braindecode">https://github.com/braindecode/braindecode</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – dataset that we will be training on</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – The hyperparameters for the model.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.EEGNet.input_size">
<span class="sig-name descname"><span class="pre">input_size</span></span><a class="headerlink" href="#woods.models.EEGNet.input_size" title="Permalink to this definition"></a></dt>
<dd><p>The size of the inputs to the model (for a single time step).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.EEGNet.output_size">
<span class="sig-name descname"><span class="pre">output_size</span></span><a class="headerlink" href="#woods.models.EEGNet.output_size" title="Permalink to this definition"></a></dt>
<dd><p>The size of the outputs of the model (number of classes).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.EEGNet.seq_len">
<span class="sig-name descname"><span class="pre">seq_len</span></span><a class="headerlink" href="#woods.models.EEGNet.seq_len" title="Permalink to this definition"></a></dt>
<dd><p>The length of the sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.models.EEGNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.EEGNet.forward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.EEGNet.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.models.EEGNet.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.models.MNIST_CNN">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">MNIST_CNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.MNIST_CNN" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Hand-tuned architecture for extracting representation from MNIST images</p>
<dl class="simple">
<dt>This was adapted from :</dt><dd><p><a class="reference external" href="https://github.com/facebookresearch/DomainBed">https://github.com/facebookresearch/DomainBed</a></p>
</dd>
</dl>
<p>In our context, it is used to extract the representation from the images which are fed to a recurrent model such as an LSTM</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – dataset that we will be training on</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – The hyperparameters for the model.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>optional</em>) – The size of the input to the model. Defaults to None. If None, the input size is calculated from the dataset.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_CNN.EMBED_DIM">
<span class="sig-name descname"><span class="pre">EMBED_DIM</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">32</span></em><a class="headerlink" href="#woods.models.MNIST_CNN.EMBED_DIM" title="Permalink to this definition"></a></dt>
<dd><p>Size of the output respresentation</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_CNN.CNN_OUT_DIM">
<span class="sig-name descname"><span class="pre">CNN_OUT_DIM</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">288</span></em><a class="headerlink" href="#woods.models.MNIST_CNN.CNN_OUT_DIM" title="Permalink to this definition"></a></dt>
<dd><p>Size of the representation after convolution, but before FCC layers</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.models.MNIST_CNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.MNIST_CNN.forward" title="Permalink to this definition"></a></dt>
<dd><p>Forward pass through the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – The input to the model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output representation of the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_CNN.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.models.MNIST_CNN.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.models.LSTM">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_hparams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.LSTM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A simple LSTM model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – dataset that we will be training on</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – The hyperparameters for the model.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>optional</em>) – The size of the input to the model. Defaults to None. If None, the input size is calculated from the dataset.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.LSTM.state_size">
<span class="sig-name descname"><span class="pre">state_size</span></span><a class="headerlink" href="#woods.models.LSTM.state_size" title="Permalink to this definition"></a></dt>
<dd><p>The size of the hidden state of the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.LSTM.recurrent_layers">
<span class="sig-name descname"><span class="pre">recurrent_layers</span></span><a class="headerlink" href="#woods.models.LSTM.recurrent_layers" title="Permalink to this definition"></a></dt>
<dd><p>The number of recurrent layers stacked on each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.LSTM.hidden_depth">
<span class="sig-name descname"><span class="pre">hidden_depth</span></span><a class="headerlink" href="#woods.models.LSTM.hidden_depth" title="Permalink to this definition"></a></dt>
<dd><p>The number of hidden layers of the classifier MLP (after LSTM).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.LSTM.hidden_width">
<span class="sig-name descname"><span class="pre">hidden_width</span></span><a class="headerlink" href="#woods.models.LSTM.hidden_width" title="Permalink to this definition"></a></dt>
<dd><p>The width of the hidden layers of the classifier MLP (after LSTM).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>All attributes need to be in the model_hparams dictionary.</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.models.LSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.LSTM.forward" title="Permalink to this definition"></a></dt>
<dd><p>Forward pass of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – The input to the model.</p></li>
<li><p><strong>time_pred</strong> (<em>torch.Tensor</em>) – The time prediction of the input.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output of the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.models.LSTM.initHidden">
<span class="sig-name descname"><span class="pre">initHidden</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.LSTM.initHidden" title="Permalink to this definition"></a></dt>
<dd><p>Initialize the hidden state of the LSTM with a normal distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the model.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device to use.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.LSTM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.models.LSTM.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">MNIST_LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_hparams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.MNIST_LSTM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A simple LSTM model taking inputs from a CNN. (see: MNIST_CNN)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – dataset that we will be training on</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – The hyperparameters for the model.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>optional</em>) – The size of the input to the model. Defaults to None. If None, the input size is calculated from the dataset.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM.state_size">
<span class="sig-name descname"><span class="pre">state_size</span></span><a class="headerlink" href="#woods.models.MNIST_LSTM.state_size" title="Permalink to this definition"></a></dt>
<dd><p>The size of the hidden state of the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM.recurrent_layers">
<span class="sig-name descname"><span class="pre">recurrent_layers</span></span><a class="headerlink" href="#woods.models.MNIST_LSTM.recurrent_layers" title="Permalink to this definition"></a></dt>
<dd><p>The number of recurrent layers stacked on each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM.hidden_depth">
<span class="sig-name descname"><span class="pre">hidden_depth</span></span><a class="headerlink" href="#woods.models.MNIST_LSTM.hidden_depth" title="Permalink to this definition"></a></dt>
<dd><p>The number of hidden layers of the classifier MLP (after LSTM).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM.hidden_width">
<span class="sig-name descname"><span class="pre">hidden_width</span></span><a class="headerlink" href="#woods.models.MNIST_LSTM.hidden_width" title="Permalink to this definition"></a></dt>
<dd><p>The width of the hidden layers of the classifier MLP (after LSTM).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>All attributes need to be in the model_hparams dictionary.</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.MNIST_LSTM.forward" title="Permalink to this definition"></a></dt>
<dd><p>Forward pass of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – The input to the model.</p></li>
<li><p><strong>time_pred</strong> (<em>torch.Tensor</em>) – The time prediction of the input.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output of the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM.initHidden">
<span class="sig-name descname"><span class="pre">initHidden</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.MNIST_LSTM.initHidden" title="Permalink to this definition"></a></dt>
<dd><p>Initialize the hidden state of the LSTM with a normal distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the model.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device to use.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.MNIST_LSTM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.models.MNIST_LSTM.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">ATTN_LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_hparams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.ATTN_LSTM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A simple LSTM model with self attention</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – dataset that we will be training on</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – The hyperparameters for the model.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>optional</em>) – The size of the input to the model. Defaults to None. If None, the input size is calculated from the dataset.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM.state_size">
<span class="sig-name descname"><span class="pre">state_size</span></span><a class="headerlink" href="#woods.models.ATTN_LSTM.state_size" title="Permalink to this definition"></a></dt>
<dd><p>The size of the hidden state of the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM.recurrent_layers">
<span class="sig-name descname"><span class="pre">recurrent_layers</span></span><a class="headerlink" href="#woods.models.ATTN_LSTM.recurrent_layers" title="Permalink to this definition"></a></dt>
<dd><p>The number of recurrent layers stacked on each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM.hidden_depth">
<span class="sig-name descname"><span class="pre">hidden_depth</span></span><a class="headerlink" href="#woods.models.ATTN_LSTM.hidden_depth" title="Permalink to this definition"></a></dt>
<dd><p>The number of hidden layers of the classifier MLP (after LSTM).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM.hidden_width">
<span class="sig-name descname"><span class="pre">hidden_width</span></span><a class="headerlink" href="#woods.models.ATTN_LSTM.hidden_width" title="Permalink to this definition"></a></dt>
<dd><p>The width of the hidden layers of the classifier MLP (after LSTM).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>All attributes need to be in the model_hparams dictionary.</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.ATTN_LSTM.forward" title="Permalink to this definition"></a></dt>
<dd><p>Forward pass of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – The input to the model.</p></li>
<li><p><strong>time_pred</strong> (<em>torch.Tensor</em>) – The time prediction of the input.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output of the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM.initHidden">
<span class="sig-name descname"><span class="pre">initHidden</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.ATTN_LSTM.initHidden" title="Permalink to this definition"></a></dt>
<dd><p>Initialize the hidden state of the LSTM with a normal distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the model.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device to use.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.ATTN_LSTM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.models.ATTN_LSTM.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.models.CRNN">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.models.</span></span><span class="sig-name descname"><span class="pre">CRNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_hparams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.CRNN" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Convolutional Recurrent Neural Network</p>
<dl class="simple">
<dt>This is taken inspired from the repository:</dt><dd><p><a class="reference external" href="https://github.com/HHTseng/video-classification/">https://github.com/HHTseng/video-classification/</a></p>
</dd>
</dl>
<p>But here we use the ResNet50 architecture pretrained on ImageNet, and we use the ATTN_LSTM model on top of the outputs of the ResNet50 to make predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – dataset that we will be training on</p></li>
<li><p><strong>model_hparams</strong> (<em>dict</em>) – The hyperparameters for the model.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.CRNN.fc_hidden1">
<span class="sig-name descname"><span class="pre">fc_hidden1</span></span><a class="headerlink" href="#woods.models.CRNN.fc_hidden1" title="Permalink to this definition"></a></dt>
<dd><p>The size of the first hidden layer of the CNN embedding.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.CRNN.fc_hidden2">
<span class="sig-name descname"><span class="pre">fc_hidden2</span></span><a class="headerlink" href="#woods.models.CRNN.fc_hidden2" title="Permalink to this definition"></a></dt>
<dd><p>The size of the second hidden layer of the CNN embedding.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.CRNN.CNN_embed_dim">
<span class="sig-name descname"><span class="pre">CNN_embed_dim</span></span><a class="headerlink" href="#woods.models.CRNN.CNN_embed_dim" title="Permalink to this definition"></a></dt>
<dd><p>The size of the CNN embedding.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.models.CRNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.models.CRNN.forward" title="Permalink to this definition"></a></dt>
<dd><p>Forward pass through CRNN
:param input: Tensor, shape [batch_size, seq_len, input_size]
:param time_pred: Tensor, time prediction indexes</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.models.CRNN.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.models.CRNN.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<span id="document-woods.objectives"></span><section id="module-woods.objectives">
<span id="woods-objectives-module"></span><h4>woods.objectives module<a class="headerlink" href="#module-woods.objectives" title="Permalink to this headline"></a></h4>
<p>Defining domain generalization algorithms</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Classes:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.objectives.ANDMask" title="woods.objectives.ANDMask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ANDMask</span></code></a></p></td>
<td><p>Learning Explanations that are Hard to Vary [<a class="reference external" href="https://arxiv.org/abs/2009.00329">https://arxiv.org/abs/2009.00329</a>] AND-Mask implementation from [<a class="reference external" href="https://github.com/gibipara92/learning-explanations-hard-to-vary">https://github.com/gibipara92/learning-explanations-hard-to-vary</a>]</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ERM</span></code></a></p></td>
<td><p>Empirical Risk Minimization (ERM)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.objectives.Fish" title="woods.objectives.Fish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Fish</span></code></a></p></td>
<td><p>Implementation of Fish, as seen in Gradient Matching for Domain Generalization, Shi et al. 2021.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.objectives.IGA" title="woods.objectives.IGA"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IGA</span></code></a></p></td>
<td><p>Inter-environmental Gradient Alignment From <a class="reference external" href="https://arxiv.org/abs/2008.01883v2">https://arxiv.org/abs/2008.01883v2</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.objectives.IRM" title="woods.objectives.IRM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IRM</span></code></a></p></td>
<td><p>Invariant Risk Minimization (IRM)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.objectives.Objective" title="woods.objectives.Objective"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Objective</span></code></a></p></td>
<td><p>A subclass of Objective implements a domain generalization Gradients.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.objectives.SANDMask" title="woods.objectives.SANDMask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SANDMask</span></code></a></p></td>
<td><p>Learning Explanations that are Hard to Vary [<a class="reference external" href="https://arxiv.org/abs/2009.00329">https://arxiv.org/abs/2009.00329</a>] AND-Mask implementation from [<a class="reference external" href="https://github.com/gibipara92/learning-explanations-hard-to-vary">https://github.com/gibipara92/learning-explanations-hard-to-vary</a>]</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.objectives.SD" title="woods.objectives.SD"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SD</span></code></a></p></td>
<td><p>Gradient Starvation: A Learning Proclivity in Neural Networks Equation 25 from [<a class="reference external" href="https://arxiv.org/pdf/2011.09468.pdf">https://arxiv.org/pdf/2011.09468.pdf</a>]</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.objectives.VREx" title="woods.objectives.VREx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VREx</span></code></a></p></td>
<td><p>V-REx Objective from <a class="reference external" href="http://arxiv.org/abs/2003.00688">http://arxiv.org/abs/2003.00688</a></p></td>
</tr>
</tbody>
</table>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.objectives.get_objective_class" title="woods.objectives.get_objective_class"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_objective_class</span></code></a></p></td>
<td><p>Return the objective class with the given name.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.objectives.get_objective_class">
<span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">get_objective_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objective_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.get_objective_class" title="Permalink to this definition"></a></dt>
<dd><p>Return the objective class with the given name.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.Objective">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">Objective</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.Objective" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A subclass of Objective implements a domain generalization Gradients.
Subclasses should implement the following:
- update
- predict</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.Objective.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">losses</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.Objective.backward" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Gradients for model update</p>
<p>Admits a list of unlabeled losses from the test domains: losses</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.Objective.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.Objective.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.ERM">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">ERM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.ERM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.Objective" title="woods.objectives.Objective"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.Objective</span></code></a></p>
<p>Empirical Risk Minimization (ERM)</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.ERM.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">all_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.ERM.predict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.ERM.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.ERM.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.ERM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.ERM.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.IRM">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">IRM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.IRM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.ERM</span></code></a></p>
<p>Invariant Risk Minimization (IRM)</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.IRM.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.IRM.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.IRM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.IRM.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.VREx">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">VREx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.VREx" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.ERM</span></code></a></p>
<p>V-REx Objective from <a class="reference external" href="http://arxiv.org/abs/2003.00688">http://arxiv.org/abs/2003.00688</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.VREx.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.VREx.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.VREx.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.VREx.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.SD">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">SD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.SD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.ERM</span></code></a></p>
<p>Gradient Starvation: A Learning Proclivity in Neural Networks
Equation 25 from [<a class="reference external" href="https://arxiv.org/pdf/2011.09468.pdf">https://arxiv.org/pdf/2011.09468.pdf</a>]</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.SD.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.SD.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.SD.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.SD.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.ANDMask">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">ANDMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.ANDMask" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.ERM</span></code></a></p>
<p>Learning Explanations that are Hard to Vary [<a class="reference external" href="https://arxiv.org/abs/2009.00329">https://arxiv.org/abs/2009.00329</a>]
AND-Mask implementation from [<a class="reference external" href="https://github.com/gibipara92/learning-explanations-hard-to-vary">https://github.com/gibipara92/learning-explanations-hard-to-vary</a>]</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.ANDMask.mask_grads">
<span class="sig-name descname"><span class="pre">mask_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tau</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.ANDMask.mask_grads" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.ANDMask.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.ANDMask.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.ANDMask.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.ANDMask.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.IGA">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">IGA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.IGA" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.ERM</span></code></a></p>
<p>Inter-environmental Gradient Alignment
From <a class="reference external" href="https://arxiv.org/abs/2008.01883v2">https://arxiv.org/abs/2008.01883v2</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.IGA.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.IGA.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.IGA.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.IGA.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.Fish">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">Fish</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.Fish" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.ERM</span></code></a></p>
<p>Implementation of Fish, as seen in Gradient Matching for Domain
Generalization, Shi et al. 2021.</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.Fish.create_copy">
<span class="sig-name descname"><span class="pre">create_copy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.Fish.create_copy" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.Fish.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.Fish.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.Fish.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.Fish.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.objectives.SANDMask">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.objectives.</span></span><span class="sig-name descname"><span class="pre">SANDMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.SANDMask" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#woods.objectives.ERM" title="woods.objectives.ERM"><code class="xref py py-class docutils literal notranslate"><span class="pre">woods.objectives.ERM</span></code></a></p>
<p>Learning Explanations that are Hard to Vary [<a class="reference external" href="https://arxiv.org/abs/2009.00329">https://arxiv.org/abs/2009.00329</a>]
AND-Mask implementation from [<a class="reference external" href="https://github.com/gibipara92/learning-explanations-hard-to-vary">https://github.com/gibipara92/learning-explanations-hard-to-vary</a>]</p>
<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.SANDMask.mask_grads">
<span class="sig-name descname"><span class="pre">mask_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tau</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.SANDMask.mask_grads" title="Permalink to this definition"></a></dt>
<dd><p>Mask are ranged in [0,1] to form a set of updates for each parameter based on the agreement
of gradients coming from different environments.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.objectives.SANDMask.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">minibatches_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.objectives.SANDMask.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="woods.objectives.SANDMask.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#woods.objectives.SANDMask.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<span id="document-woods.train"></span><section id="module-woods.train">
<span id="woods-train-module"></span><h4>woods.train module<a class="headerlink" href="#module-woods.train" title="Permalink to this headline"></a></h4>
<p>Defining the training functions that are used to train and evaluate models</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.train.get_accuracies" title="woods.train.get_accuracies"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_accuracies</span></code></a></p></td>
<td><p>Get accuracies for all splits using fast loaders</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.train.get_split_accuracy_seq" title="woods.train.get_split_accuracy_seq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_split_accuracy_seq</span></code></a></p></td>
<td><p>Get accuracy and loss for a dataset that is of the <cite>seq</cite> setup</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.train.get_split_accuracy_step" title="woods.train.get_split_accuracy_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_split_accuracy_step</span></code></a></p></td>
<td><p>Get accuracy and loss for a dataset that is of the <cite>step</cite> setup</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.train.train" title="woods.train.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a></p></td>
<td><p>Train a model on a given dataset with a given objective</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.train.train_step" title="woods.train.train_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_step</span></code></a></p></td>
<td><p>Train a single training step for a model</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.train.train_step">
<span class="sig-prename descclassname"><span class="pre">woods.train.</span></span><span class="sig-name descname"><span class="pre">train_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_loaders_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.train.train_step" title="Permalink to this definition"></a></dt>
<dd><p>Train a single training step for a model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – nn model defined in a models.py</p></li>
<li><p><strong>objective</strong> – objective we are using for training</p></li>
<li><p><strong>dataset</strong> – dataset object we are training on</p></li>
<li><p><strong>in_loaders_iter</strong> – iterable of iterable of data loaders</p></li>
<li><p><strong>device</strong> – device on which we are training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.train.train">
<span class="sig-prename descclassname"><span class="pre">woods.train.</span></span><span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_hparams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.train.train" title="Permalink to this definition"></a></dt>
<dd><p>Train a model on a given dataset with a given objective</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> – flags from argparse</p></li>
<li><p><strong>training_hparams</strong> – training hyperparameters</p></li>
<li><p><strong>model</strong> – nn model defined in a models.py</p></li>
<li><p><strong>objective</strong> – objective we are using for training</p></li>
<li><p><strong>dataset</strong> – dataset object we are training on</p></li>
<li><p><strong>device</strong> – device on which we are training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.train.get_accuracies">
<span class="sig-prename descclassname"><span class="pre">woods.train.</span></span><span class="sig-name descname"><span class="pre">get_accuracies</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.train.get_accuracies" title="Permalink to this definition"></a></dt>
<dd><p>Get accuracies for all splits using fast loaders</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>objective</strong> – objective we are using for training</p></li>
<li><p><strong>dataset</strong> – dataset object we are training on</p></li>
<li><p><strong>device</strong> – device on which we are training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.train.get_split_accuracy_seq">
<span class="sig-prename descclassname"><span class="pre">woods.train.</span></span><span class="sig-name descname"><span class="pre">get_split_accuracy_seq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.train.get_split_accuracy_seq" title="Permalink to this definition"></a></dt>
<dd><p>Get accuracy and loss for a dataset that is of the <cite>seq</cite> setup</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>objective</strong> – objective we are using for training</p></li>
<li><p><strong>dataset</strong> – dataset object we are training on</p></li>
<li><p><strong>loader</strong> – data loader of which we want the accuracy</p></li>
<li><p><strong>device</strong> – device on which we are training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.train.get_split_accuracy_step">
<span class="sig-prename descclassname"><span class="pre">woods.train.</span></span><span class="sig-name descname"><span class="pre">get_split_accuracy_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.train.get_split_accuracy_step" title="Permalink to this definition"></a></dt>
<dd><p>Get accuracy and loss for a dataset that is of the <cite>step</cite> setup</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>objective</strong> – objective we are using for training</p></li>
<li><p><strong>dataset</strong> – dataset object we are training on</p></li>
<li><p><strong>loader</strong> – data loader of which we want the accuracy</p></li>
<li><p><strong>device</strong> – device on which we are training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<span id="document-woods.utils"></span><section id="module-woods.utils">
<span id="woods-utils-module"></span><h4>woods.utils module<a class="headerlink" href="#module-woods.utils" title="Permalink to this headline"></a></h4>
<p>Set of utility functions used throughout the package</p>
<section id="summary">
<h5>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h5>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.utils.check_file_integrity" title="woods.utils.check_file_integrity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">check_file_integrity</span></code></a></p></td>
<td><p>Check for integrity of files from a hyper parameter sweep</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.utils.get_cmap" title="woods.utils.get_cmap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_cmap</span></code></a></p></td>
<td><p>Returns a function that maps each index in 0, 1, ..., n-1 to a distinct RGB color; the keyword argument name must be a standard mpl colormap name.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.utils.get_job_name" title="woods.utils.get_job_name"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_job_name</span></code></a></p></td>
<td><p>Generates the name of the output file for a training run as a function of the config</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.utils.get_latex_table" title="woods.utils.get_latex_table"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_latex_table</span></code></a></p></td>
<td><p>Construct and export a LaTeX table from a PrettyTable.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.utils.plot_results" title="woods.utils.plot_results"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_results</span></code></a></p></td>
<td><p>Plot results - accuracy and loss - w.r.t.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.utils.print_results" title="woods.utils.print_results"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_results</span></code></a></p></td>
<td><p>Print results from a results json file :param results_path: path to a results json file coming from a training run :type results_path: str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.utils.setup_pretty_table" title="woods.utils.setup_pretty_table"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup_pretty_table</span></code></a></p></td>
<td><p>Setup the printed table that show the results at each checkpoints</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h5>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="woods.utils.get_cmap">
<span class="sig-prename descclassname"><span class="pre">woods.utils.</span></span><span class="sig-name descname"><span class="pre">get_cmap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hsv'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.utils.get_cmap" title="Permalink to this definition"></a></dt>
<dd><p>Returns a function that maps each index in 0, 1, …, n-1 to a distinct
RGB color; the keyword argument name must be a standard mpl colormap name.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.utils.plot_results">
<span class="sig-prename descclassname"><span class="pre">woods.utils.</span></span><span class="sig-name descname"><span class="pre">plot_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.utils.plot_results" title="Permalink to this definition"></a></dt>
<dd><p>Plot results - accuracy and loss - w.r.t. training step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>results_path</strong> (<em>str</em>) – path to a results json file coming from a training run</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.utils.print_results">
<span class="sig-prename descclassname"><span class="pre">woods.utils.</span></span><span class="sig-name descname"><span class="pre">print_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.utils.print_results" title="Permalink to this definition"></a></dt>
<dd><p>Print results from a results json file
:param results_path: path to a results json file coming from a training run
:type results_path: str</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.utils.get_job_name">
<span class="sig-prename descclassname"><span class="pre">woods.utils.</span></span><span class="sig-name descname"><span class="pre">get_job_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.utils.get_job_name" title="Permalink to this definition"></a></dt>
<dd><p>Generates the name of the output file for a training run as a function of the config</p>
<p>Seq setup:
&lt;objective&gt;_&lt;dataset&gt;_&lt;test_env&gt;_H&lt;hparams_seed&gt;_T&lt;trial_seed&gt;.json
Step setup:
&lt;objective&gt;_&lt;dataset&gt;_&lt;test_env&gt;_H&lt;hparams_seed&gt;_T&lt;trial_seed&gt;_S&lt;test_step&gt;.json</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>dict</em>) – dictionnary of the config for a training run</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>name of the output json file of the training run</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.utils.check_file_integrity">
<span class="sig-prename descclassname"><span class="pre">woods.utils.</span></span><span class="sig-name descname"><span class="pre">check_file_integrity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results_dir</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.utils.check_file_integrity" title="Permalink to this definition"></a></dt>
<dd><p>Check for integrity of files from a hyper parameter sweep</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>results_dir</strong> (<em>str</em>) – directory where sweep results are stored</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AssertionError</strong> – If there is a sweep file missing</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.utils.setup_pretty_table">
<span class="sig-prename descclassname"><span class="pre">woods.utils.</span></span><span class="sig-name descname"><span class="pre">setup_pretty_table</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.utils.setup_pretty_table" title="Permalink to this definition"></a></dt>
<dd><p>Setup the printed table that show the results at each checkpoints</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flags</strong> (<em>Namespace</em>) – Namespace of the argparser containing the config of the training run</p></li>
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#woods.datasets.Multi_Domain_Dataset" title="woods.datasets.Multi_Domain_Dataset"><em>Multi_Domain_Dataset</em></a>) – Dataset Object</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an instance of prettytable.PrettyTable</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>PrettyTable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.utils.get_latex_table">
<span class="sig-prename descclassname"><span class="pre">woods.utils.</span></span><span class="sig-name descname"><span class="pre">get_latex_table</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">table</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">caption</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.utils.get_latex_table" title="Permalink to this definition"></a></dt>
<dd><p>Construct and export a LaTeX table from a PrettyTable.</p>
<p>Inspired from : <a class="reference external" href="https://github.com/adasilva/prettytable">https://github.com/adasilva/prettytable</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>table</strong> (<em>PrettyTable</em>) – </p></li>
<li><p><strong>caption</strong> (<em>str</em><em>, </em><em>optional</em>) – a caption for the table. Defaults to None.</p></li>
<li><p><strong>label</strong> (<em>str</em><em>, </em><em>optional</em>) – a latex reference tag. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>printable latex string</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</div>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.command_launchers" title="woods.command_launchers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.command_launchers</span></code></a></p></td>
<td><p>Set of functions used to launch lists of python scripts</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.datasets" title="woods.datasets"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.datasets</span></code></a></p></td>
<td><p>Defining the benchmarks for OoD generalization in time-series</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.hyperparams" title="woods.hyperparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.hyperparams</span></code></a></p></td>
<td><p>Defining hyper parameters and their distributions for HPO</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.model_selection" title="woods.model_selection"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.model_selection</span></code></a></p></td>
<td><p>Defining the model selection strategies</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.models" title="woods.models"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.models</span></code></a></p></td>
<td><p>Defining the architectures used for benchmarking algorithms</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.objectives" title="woods.objectives"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.objectives</span></code></a></p></td>
<td><p>Defining domain generalization algorithms</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.train" title="woods.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.train</span></code></a></p></td>
<td><p>Defining the training functions that are used to train and evaluate models</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.utils" title="woods.utils"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.utils</span></code></a></p></td>
<td><p>Set of utility functions used throughout the package</p></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper compound">
<span id="document-woods.scripts"></span><section id="module-woods.scripts">
<span id="woods-scripts"></span><h4>woods.scripts<a class="headerlink" href="#module-woods.scripts" title="Permalink to this headline"></a></h4>
<div class="toctree-wrapper compound">
<span id="document-woods.scripts.compile_results"></span><section id="module-woods.scripts.compile_results">
<span id="woods-scripts-compile-results-module"></span><h5>woods.scripts.compile_results module<a class="headerlink" href="#module-woods.scripts.compile_results" title="Permalink to this headline"></a></h5>
<p>Compile resuls from a hyperparameter sweep and perform model selection strategies</p>
<p>See <a class="reference external" href="https://woods.readthedocs.io/en/latest/running_a_sweep.html">https://woods.readthedocs.io/en/latest/running_a_sweep.html</a> to learn more about usage.</p>
</section>
<span id="document-woods.scripts.download"></span><section id="module-woods.scripts.download">
<span id="woods-scripts-download-module"></span><h5>woods.scripts.download module<a class="headerlink" href="#module-woods.scripts.download" title="Permalink to this headline"></a></h5>
<p>Directly download the preprocessed data</p>
<section id="summary">
<h6>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h6>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.scripts.download.CAP" title="woods.scripts.download.CAP"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CAP</span></code></a></p></td>
<td><p>Download the CAP dataset</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.scripts.download.HHAR" title="woods.scripts.download.HHAR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HHAR</span></code></a></p></td>
<td><p>Download the HHAR dataset</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.scripts.download.LSA64" title="woods.scripts.download.LSA64"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSA64</span></code></a></p></td>
<td><p>Download the LSA64 dataset</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.scripts.download.PCL" title="woods.scripts.download.PCL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PCL</span></code></a></p></td>
<td><p>Download the PCL dataset</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.scripts.download.SEDFx" title="woods.scripts.download.SEDFx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SEDFx</span></code></a></p></td>
<td><p>Download the SEDFx dataset</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h6>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h6>
<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.download.CAP">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.download.</span></span><span class="sig-name descname"><span class="pre">CAP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.download.CAP" title="Permalink to this definition"></a></dt>
<dd><p>Download the CAP dataset</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.download.SEDFx">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.download.</span></span><span class="sig-name descname"><span class="pre">SEDFx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.download.SEDFx" title="Permalink to this definition"></a></dt>
<dd><p>Download the SEDFx dataset</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.download.PCL">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.download.</span></span><span class="sig-name descname"><span class="pre">PCL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.download.PCL" title="Permalink to this definition"></a></dt>
<dd><p>Download the PCL dataset</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.download.HHAR">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.download.</span></span><span class="sig-name descname"><span class="pre">HHAR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.download.HHAR" title="Permalink to this definition"></a></dt>
<dd><p>Download the HHAR dataset</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.download.LSA64">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.download.</span></span><span class="sig-name descname"><span class="pre">LSA64</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.download.LSA64" title="Permalink to this definition"></a></dt>
<dd><p>Download the LSA64 dataset</p>
</dd></dl>

</section>
</section>
<span id="document-woods.scripts.fetch_and_preprocess"></span><section id="module-woods.scripts.fetch_and_preprocess">
<span id="woods-scripts-fetch-and-preprocess-module"></span><h5>woods.scripts.fetch_and_preprocess module<a class="headerlink" href="#module-woods.scripts.fetch_and_preprocess" title="Permalink to this headline"></a></h5>
<p>This module is used to run yourself the raw download and preprocessing of the data</p>
<p>You can directly download the preprocessed data with the download.py module.
This module is used only for transparancy of how the datasets are preprocessed.
It also gives the opportunity to the most curageous to change the preprocessing approaches of the data for curiosity.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The intention of releasing the benchmarks of woods is to investigate the performance of domain generalization techniques.
Although some preprocessing tricks could lead to better OoD performance, this approach is not encouraged when using the WOODS benchmarks.</p>
</div>
<section id="summary">
<h6>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h6>
<p>Classes:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.scripts.fetch_and_preprocess.CAP" title="woods.scripts.fetch_and_preprocess.CAP"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CAP</span></code></a></p></td>
<td><p>Fetch the data from the PhysioNet website and preprocess it</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.scripts.fetch_and_preprocess.PCL" title="woods.scripts.fetch_and_preprocess.PCL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PCL</span></code></a></p></td>
<td><p>Fetch the data using moabb and preprocess it</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.scripts.fetch_and_preprocess.SEDFx" title="woods.scripts.fetch_and_preprocess.SEDFx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SEDFx</span></code></a></p></td>
<td><p>Fetch the PhysioNet Sleep-EDF Database Expanded Dataset and preprocess it</p></td>
</tr>
</tbody>
</table>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.scripts.fetch_and_preprocess.HHAR" title="woods.scripts.fetch_and_preprocess.HHAR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HHAR</span></code></a></p></td>
<td><p>Fetch and preprocess the HHAR dataset</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#woods.scripts.fetch_and_preprocess.LSA64" title="woods.scripts.fetch_and_preprocess.LSA64"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSA64</span></code></a></p></td>
<td><p>Fetch the LSA64 dataset and preprocess it</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h6>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.CAP">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.scripts.fetch_and_preprocess.</span></span><span class="sig-name descname"><span class="pre">CAP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.CAP" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Fetch the data from the PhysioNet website and preprocess it</p>
<p>The download is automatic but if you want to manually download:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="o">-</span><span class="n">r</span> <span class="o">-</span><span class="n">N</span> <span class="o">-</span><span class="n">c</span> <span class="o">-</span><span class="n">np</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">physionet</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">files</span><span class="o">/</span><span class="n">capslpdb</span><span class="o">/</span><span class="mf">1.0.0</span><span class="o">/</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>argparse.Namespace</em>) – The flags of the script</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.CAP.files">
<span class="sig-name descname"><span class="pre">files</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">[['physionet.org/files/capslpdb/1.0.0/nfle29',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle7',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle1',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle5',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/n11',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/rbd18',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/plm9',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle35',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle36',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle2',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle38',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle39',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle21'],</span> <span class="pre">['physionet.org/files/capslpdb/1.0.0/nfle10',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle11',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle19',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle26',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle23'],</span> <span class="pre">['physionet.org/files/capslpdb/1.0.0/rbd8',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/rbd5',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/rbd11',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/ins8',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/rbd10'],</span> <span class="pre">['physionet.org/files/capslpdb/1.0.0/n3',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle30',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle13',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle18',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle24',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle4',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle14',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle22',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/n5',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle37'],</span> <span class="pre">['physionet.org/files/capslpdb/1.0.0/nfle3',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle40',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle15',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle12',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle28',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle34',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle16',</span> <span class="pre">'physionet.org/files/capslpdb/1.0.0/nfle17']]</span></em><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.CAP.files" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.CAP.remove_useless">
<span class="sig-name descname"><span class="pre">remove_useless</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.CAP.remove_useless" title="Permalink to this definition"></a></dt>
<dd><p>Remove useless files</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.CAP.string_2_label">
<span class="sig-name descname"><span class="pre">string_2_label</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">string</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.CAP.string_2_label" title="Permalink to this definition"></a></dt>
<dd><p>Convert string to label</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.CAP.read_annotation">
<span class="sig-name descname"><span class="pre">read_annotation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">txt_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.CAP.read_annotation" title="Permalink to this definition"></a></dt>
<dd><p>Read annotation file for the CAP dataset</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.CAP.gather_EEG">
<span class="sig-name descname"><span class="pre">gather_EEG</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.CAP.gather_EEG" title="Permalink to this definition"></a></dt>
<dd><p>Gets the intersection of common channels across all machines</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of channels (strings)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.SEDFx">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.scripts.fetch_and_preprocess.</span></span><span class="sig-name descname"><span class="pre">SEDFx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.SEDFx" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Fetch the PhysioNet Sleep-EDF Database Expanded Dataset and preprocess it</p>
<p>The download is automatic but if you want to manually download:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="o">-</span><span class="n">r</span> <span class="o">-</span><span class="n">N</span> <span class="o">-</span><span class="n">c</span> <span class="o">-</span><span class="n">np</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">physionet</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">files</span><span class="o">/</span><span class="n">sleep</span><span class="o">-</span><span class="n">edfx</span><span class="o">/</span><span class="mf">1.0.0</span><span class="o">/</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>argparse.Namespace</em>) – The flags of the script</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.SEDFx.remove_useless">
<span class="sig-name descname"><span class="pre">remove_useless</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.SEDFx.remove_useless" title="Permalink to this definition"></a></dt>
<dd><p>Remove useless files</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.SEDFx.string_2_label">
<span class="sig-name descname"><span class="pre">string_2_label</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">string</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.SEDFx.string_2_label" title="Permalink to this definition"></a></dt>
<dd><p>Convert string to label</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.SEDFx.read_annotation">
<span class="sig-name descname"><span class="pre">read_annotation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">txt_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.SEDFx.read_annotation" title="Permalink to this definition"></a></dt>
<dd><p>Read annotation file</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.SEDFx.gather_EEG">
<span class="sig-name descname"><span class="pre">gather_EEG</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.SEDFx.gather_EEG" title="Permalink to this definition"></a></dt>
<dd><p>Gets the intersection of common channels across all machines</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of channels (strings)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.HHAR">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.fetch_and_preprocess.</span></span><span class="sig-name descname"><span class="pre">HHAR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.HHAR" title="Permalink to this definition"></a></dt>
<dd><p>Fetch and preprocess the HHAR dataset</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to manually download the HHAR dataset from the source and place it in the data folder in order to preprocess it yourself:</p>
<blockquote>
<div><p><a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition">https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition</a></p>
</div></blockquote>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>argparse.Namespace</em>) – The flags of the script</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.LSA64">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.fetch_and_preprocess.</span></span><span class="sig-name descname"><span class="pre">LSA64</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.LSA64" title="Permalink to this definition"></a></dt>
<dd><p>Fetch the LSA64 dataset and preprocess it</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to manually download the HHAR dataset from the source and place it in the data folder in order to preprocess it yourself:</p>
<blockquote>
<div><p><a class="reference external" href="https://mega.nz/file/FQJGCYba#uJKGKLW1VlpCpLCrGVu89wyQnm9b4sKquCOEAjW5zMo">https://mega.nz/file/FQJGCYba#uJKGKLW1VlpCpLCrGVu89wyQnm9b4sKquCOEAjW5zMo</a></p>
</div></blockquote>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>argparse.Namespace</em>) – The flags of the script</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.PCL">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">woods.scripts.fetch_and_preprocess.</span></span><span class="sig-name descname"><span class="pre">PCL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.PCL" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Fetch the data using moabb and preprocess it</p>
<dl class="simple">
<dt>Source of MOABB:</dt><dd><p><a class="reference external" href="http://moabb.neurotechx.com/docs/index.html">http://moabb.neurotechx.com/docs/index.html</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>argparse.Namespace</em>) – The flags of the script</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is hell to run. It takes a while to download and requires a lot of RAM.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="woods.scripts.fetch_and_preprocess.PCL.relabel">
<span class="sig-name descname"><span class="pre">relabel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">l</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.fetch_and_preprocess.PCL.relabel" title="Permalink to this definition"></a></dt>
<dd><p>Converts labels from str to int</p>
</dd></dl>

</dd></dl>

</section>
</section>
<span id="document-woods.scripts.hparams_sweep"></span><section id="module-woods.scripts.hparams_sweep">
<span id="woods-scripts-hparams-sweep-module"></span><h5>woods.scripts.hparams_sweep module<a class="headerlink" href="#module-woods.scripts.hparams_sweep" title="Permalink to this headline"></a></h5>
<p>Perform an hyper parameter sweep</p>
<p>See <a class="reference external" href="https://woods.readthedocs.io/en/latest/running_a_sweep.html">https://woods.readthedocs.io/en/latest/running_a_sweep.html</a> for usage.</p>
<section id="summary">
<h6>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h6>
<p>Functions:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#woods.scripts.hparams_sweep.make_args_list" title="woods.scripts.hparams_sweep.make_args_list"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_args_list</span></code></a></p></td>
<td><p>Creates a list of commands to launch all of the training runs in the hyper parameter sweep</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h6>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h6>
<dl class="py function">
<dt class="sig sig-object py" id="woods.scripts.hparams_sweep.make_args_list">
<span class="sig-prename descclassname"><span class="pre">woods.scripts.hparams_sweep.</span></span><span class="sig-name descname"><span class="pre">make_args_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#woods.scripts.hparams_sweep.make_args_list" title="Permalink to this definition"></a></dt>
<dd><p>Creates a list of commands to launch all of the training runs in the hyper parameter sweep</p>
<p>Heavily inspired from <a class="reference external" href="https://github.com/facebookresearch/DomainBed/blob/9e864cc4057d1678765ab3ecb10ae37a4c75a840/domainbed/scripts/sweep.py#L98">https://github.com/facebookresearch/DomainBed/blob/9e864cc4057d1678765ab3ecb10ae37a4c75a840/domainbed/scripts/sweep.py#L98</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>flags</strong> (<em>dict</em>) – arguments of the hyper parameter sweep</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of strings terminal commands that calls the training runs of the sweep
list: list of dict where dicts are the arguments for the training runs of the sweep</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<span id="document-woods.scripts.main"></span><section id="module-woods.scripts.main">
<span id="woods-scripts-main-module"></span><h5>woods.scripts.main module<a class="headerlink" href="#module-woods.scripts.main" title="Permalink to this headline"></a></h5>
<p>Script used for the main functionnalities of the woods package</p>
<dl class="simple">
<dt>There is 2 mode of operation:</dt><dd><ul class="simple">
<li><p>training mode: trains a model on a given dataset with a given test environment using a given algorithm</p></li>
<li><p>test mode: tests an existing model on a given dataset with a given test environment using a given algorithm</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">raises NotImplementedError</dt>
<dd class="field-odd"><p>Some part of the code is not implemented yet</p>
</dd>
</dl>
</section>
<span id="document-woods.scripts.visualize_results"></span><section id="module-woods.scripts.visualize_results">
<span id="woods-scripts-visualize-results-module"></span><h5>woods.scripts.visualize_results module<a class="headerlink" href="#module-woods.scripts.visualize_results" title="Permalink to this headline"></a></h5>
<p>Visualize logs from a training run</p>
</section>
</div>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts.compile_results" title="woods.scripts.compile_results"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.compile_results</span></code></a></p></td>
<td><p>Compile resuls from a hyperparameter sweep and perform model selection strategies</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.scripts.download" title="woods.scripts.download"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.download</span></code></a></p></td>
<td><p>Directly download the preprocessed data</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts.fetch_and_preprocess" title="woods.scripts.fetch_and_preprocess"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.fetch_and_preprocess</span></code></a></p></td>
<td><p>This module is used to run yourself the raw download and preprocessing of the data</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.scripts.hparams_sweep" title="woods.scripts.hparams_sweep"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.hparams_sweep</span></code></a></p></td>
<td><p>Perform an hyper parameter sweep</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts.main" title="woods.scripts.main"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.main</span></code></a></p></td>
<td><p>Script used for the main functionnalities of the woods package</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.scripts.visualize_results" title="woods.scripts.visualize_results"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.visualize_results</span></code></a></p></td>
<td><p>Visualize logs from a training run</p></td>
</tr>
</tbody>
</table>
</section>
</div>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts" title="woods.scripts"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</section>
</div>
<section id="library">
<h3>Library<a class="headerlink" href="#library" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods" title="woods"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.objectives" title="woods.objectives"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.objectives</span></code></a></p></td>
<td><p>Defining domain generalization algorithms</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.datasets" title="woods.datasets"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.datasets</span></code></a></p></td>
<td><p>Defining the benchmarks for OoD generalization in time-series</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.hyperparams" title="woods.hyperparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.hyperparams</span></code></a></p></td>
<td><p>Defining hyper parameters and their distributions for HPO</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.train" title="woods.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.train</span></code></a></p></td>
<td><p>Defining the training functions that are used to train and evaluate models</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.models" title="woods.models"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.models</span></code></a></p></td>
<td><p>Defining the architectures used for benchmarking algorithms</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.model_selection" title="woods.model_selection"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.model_selection</span></code></a></p></td>
<td><p>Defining the model selection strategies</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.command_launchers" title="woods.command_launchers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.command_launchers</span></code></a></p></td>
<td><p>Set of functions used to launch lists of python scripts</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.utils" title="woods.utils"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.utils</span></code></a></p></td>
<td><p>Set of utility functions used throughout the package</p></td>
</tr>
</tbody>
</table>
</section>
<section id="scripts">
<h3>Scripts<a class="headerlink" href="#scripts" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts" title="woods.scripts"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.scripts.compile_results" title="woods.scripts.compile_results"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.compile_results</span></code></a></p></td>
<td><p>Compile resuls from a hyperparameter sweep and perform model selection strategies</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts.download" title="woods.scripts.download"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.download</span></code></a></p></td>
<td><p>Directly download the preprocessed data</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.scripts.fetch_and_preprocess" title="woods.scripts.fetch_and_preprocess"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.fetch_and_preprocess</span></code></a></p></td>
<td><p>This module is used to run yourself the raw download and preprocessing of the data</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts.hparams_sweep" title="woods.scripts.hparams_sweep"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.hparams_sweep</span></code></a></p></td>
<td><p>Perform an hyper parameter sweep</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="index.html#module-woods.scripts.main" title="woods.scripts.main"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.main</span></code></a></p></td>
<td><p>Script used for the main functionnalities of the woods package</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="index.html#module-woods.scripts.visualize_results" title="woods.scripts.visualize_results"><code class="xref py py-obj docutils literal notranslate"><span class="pre">woods.scripts.visualize_results</span></code></a></p></td>
<td><p>Visualize logs from a training run</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline"></a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Jean-Christophe Gagnon-Audet.
      <span class="commit">Revision <code>bcb83a71</code>.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="Versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="//woods.readthedocs.io/_/downloads/en/latest/htmlzip/">html</a></dd>
        
          <dd><a href="//woods.readthedocs.io/_/downloads/en/latest/epub/">epub</a></dd>
        
      </dl>
      <dl>
        
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/woods/?fromdocs=woods">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/woods/?fromdocs=woods">Builds</a>
          </dd>
      </dl>
    </div>
  </div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>